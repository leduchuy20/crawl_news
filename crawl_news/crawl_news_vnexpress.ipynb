{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7d8e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://vnexpress.net/giao-duc] added 766\n",
      "[https://vnexpress.net/du-lich] added 834\n",
      "[https://vnexpress.net/oto-xe-may] added 1036\n",
      "[https://vnexpress.net/doi-song] added 822\n",
      "Done. Total appended 3458 rows to vnexpress_html_categories_vi_v2.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# thay vì zoneinfo:\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    # \"https://vnexpress.net/the-gioi\",\n",
    "    # \"https://vnexpress.net/thoi-su\",\n",
    "    # \"https://vnexpress.net/kinh-doanh\",\n",
    "    # \"https://vnexpress.net/khoa-hoc-cong-nghe\",\n",
    "    # \"https://vnexpress.net/goc-nhin\",\n",
    "    # \"https://vnexpress.net/bat-dong-san\",\n",
    "    # \"https://vnexpress.net/suc-khoe\",\n",
    "    # \"https://vnexpress.net/the-thao\",\n",
    "    # \"https://vnexpress.net/giai-tri\",\n",
    "    # \"https://vnexpress.net/phap-luat\",\n",
    "    \"https://vnexpress.net/giao-duc\",\n",
    "    \"https://vnexpress.net/du-lich\",\n",
    "    \"https://vnexpress.net/oto-xe-may\",\n",
    "    \"https://vnexpress.net/doi-song\"\n",
    "]\n",
    "\n",
    "# Crawl từ mới -> cũ cho tới khi bài có ngày < END_DATE (theo giờ VN)\n",
    "END_DATE = \"2025-11-01\"  # YYYY-MM-DD\n",
    "MAX_PAGES_PER_CATEGORY = 200  # safety stop\n",
    "\n",
    "CSV_PATH = \"vnexpress_html_categories_vi_v2.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; VnExpressHTMLCrawler/1.0)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"VnExpress\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def is_english_site(url: str) -> bool:\n",
    "    return bool(url) and (\n",
    "        url.startswith(\"https://e.vnexpress.net/\") or url.startswith(\"http://e.vnexpress.net/\")\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def to_iso_utc(s: Optional[str]) -> Optional[str]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(s)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            # nếu không có tz, coi như giờ VN (thực tế VnExpress thường có tz trong meta)\n",
    "            if VN_TZ:\n",
    "                dt = dt.replace(tzinfo=VN_TZ)\n",
    "            else:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    iso_utc -> YYYY-MM-DD theo giờ VN để so sánh với END_DATE.\n",
    "    \"\"\"\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(iso_utc)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        if VN_TZ:\n",
    "            dt_local = dt.astimezone(VN_TZ)\n",
    "        else:\n",
    "            dt_local = dt\n",
    "        return dt_local.date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def extract_language_from_html(soup: BeautifulSoup) -> str:\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            lang = lang.lower().strip()\n",
    "            if lang.startswith(\"vi\"):\n",
    "                return \"vi\"\n",
    "            if lang.startswith(\"en\"):\n",
    "                return \"en\"\n",
    "            return lang\n",
    "    return DEFAULT_LANGUAGE\n",
    "\n",
    "\n",
    "def extract_keywords_from_html(soup: BeautifulSoup) -> List[str]:\n",
    "    for sel in ['meta[name=\"keywords\"]', 'meta[name=\"news_keywords\"]']:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag and tag.get(\"content\"):\n",
    "            raw = tag[\"content\"]\n",
    "            kws = [x.strip() for x in raw.split(\",\") if x.strip()]\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for k in kws:\n",
    "                if k not in seen:\n",
    "                    seen.add(k)\n",
    "                    out.append(k)\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1.title-detail, h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = to_iso_utc(m_pub[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        m2 = soup.select_one('meta[itemprop=\"datePublished\"]')\n",
    "        if m2 and m2.get(\"content\"):\n",
    "            pub = to_iso_utc(m2[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        ttag = soup.select_one(\"time\")\n",
    "        if ttag:\n",
    "            pub = to_iso_utc(ttag.get(\"datetime\") or ttag.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category.primary (fallback từ meta)\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    language = extract_language_from_html(soup)\n",
    "    keywords = extract_keywords_from_html(soup)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    urls = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://vnexpress.net\" + href\n",
    "        if not href.startswith(\"https://vnexpress.net/\"):\n",
    "            continue\n",
    "        if is_english_site(href):\n",
    "            continue\n",
    "        # bài viết thường kết thúc .html\n",
    "        if \".html\" not in href:\n",
    "            continue\n",
    "        # loại các link tracking/ảnh/video nếu cần\n",
    "        urls.append(href.split(\"?\")[0])\n",
    "\n",
    "    # unique giữ thứ tự\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Cố gắng tìm link trang kế tiếp.\n",
    "    VnExpress có thể dùng ?p=2 hoặc /p2 hoặc cấu trúc khác tùy thời điểm.\n",
    "    Nếu không tìm được thì fallback theo quy ước ?p=...\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # thử rel=next\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        href = ln[\"href\"].strip()\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://vnexpress.net\" + href\n",
    "        return href\n",
    "\n",
    "    # thử anchor có text \"Sau\" / \"Tiếp\" / class pagination\n",
    "    for sel in ['a.next', 'a[rel=\"next\"]', '.pagination a.next', 'a:contains(\"Tiếp\")', 'a:contains(\"Sau\")']:\n",
    "        # BeautifulSoup không hỗ trợ :contains chuẩn, nên chỉ dùng selector đơn giản\n",
    "        pass\n",
    "\n",
    "    # fallback: thử pattern ?p=\n",
    "    # nếu category_url đã có ?p= thì tăng lên\n",
    "    if \"?p=\" in category_url:\n",
    "        return re.sub(r\"\\?p=\\d+\", f\"?p={current_page+1}\", category_url)\n",
    "    return f\"{category_url}?p={current_page+1}\"\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    id_ = md5_id(url)\n",
    "    category_primary = meta.get(\"category_from_article\") or category_fallback\n",
    "    keywords_str = \"|\".join(meta.get(\"keywords\") or [])\n",
    "    entities_str = \"|\".join(meta.get(\"entities\") or [])\n",
    "\n",
    "    return {\n",
    "        \"id\": id_,\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": category_primary or \"\",\n",
    "        \"keywords\": keywords_str,\n",
    "        \"entities\": entities_str,\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> int:\n",
    "    \"\"\"\n",
    "    Crawl 1 chuyên mục từ mới -> cũ đến khi < end_date (YYYY-MM-DD, theo giờ VN).\n",
    "    \"\"\"\n",
    "    added = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "\n",
    "    # dùng slug của category làm fallback category.primary\n",
    "    category_slug = category_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    while page <= MAX_PAGES_PER_CATEGORY and url_page:\n",
    "        html = fetch_text(url_page)\n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if DEBUG:\n",
    "            log(f\"[{category_slug}] page {page} got {len(article_urls)} candidate urls: {url_page}\")\n",
    "\n",
    "        if not article_urls:\n",
    "            # không có bài -> dừng\n",
    "            break\n",
    "\n",
    "        page_has_any_new = False\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            if is_english_site(aurl):\n",
    "                continue\n",
    "\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "\n",
    "            # fetch article\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # nếu có ngày và nhỏ hơn end_date => đánh dấu cũ\n",
    "            if pub_local_date and pub_local_date < end_date:\n",
    "                # bài này cũ hơn end_date\n",
    "                # vẫn coi là old\n",
    "                pass\n",
    "            else:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # Nếu bài >= end_date thì ghi\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "                page_has_any_new = True\n",
    "\n",
    "        # Nếu cả trang toàn bài cũ hơn end_date thì dừng category này\n",
    "        if page_all_older_than_end:\n",
    "            if DEBUG:\n",
    "                log(f\"[{category_slug}] stop: page {page} all older than end_date={end_date}\")\n",
    "            break\n",
    "\n",
    "        # đi trang tiếp\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if next_url == url_page:\n",
    "            break\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"[{cat}] added {added}\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ERROR: {e}\")\n",
    "\n",
    "    print(f\"Done. Total appended {total} rows to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
