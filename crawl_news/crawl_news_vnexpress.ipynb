{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7d8e98",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 477\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Total appended \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 477\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 467\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m CATEGORY_URLS:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         added \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    469\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m added\n",
      "Cell \u001b[1;32mIn[4], line 416\u001b[0m, in \u001b[0;36mcrawl_category\u001b[1;34m(category_url, end_date, seen_urls, seen_ids)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    415\u001b[0m     ah \u001b[38;5;241m=\u001b[39m fetch_text(aurl)\n\u001b[1;32m--> 416\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mextract_article_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mah\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] article fetch failed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maurl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 228\u001b[0m, in \u001b[0;36mextract_article_meta\u001b[1;34m(article_html)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_article_meta\u001b[39m(article_html: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 228\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# title\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\bs4\\__init__.py:476\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\bs4\\__init__.py:661\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\bs4\\builder\\_lxml.py:494\u001b[0m, in \u001b[0;36mLXMLTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser_for(encoding)\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m, etree\u001b[38;5;241m.\u001b[39mParserError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1361\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parser.pxi:1481\u001b[0m, in \u001b[0;36mlxml.etree._FeedParser.feed\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parsertarget.pxi:161\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parsertarget.pxi:156\u001b[0m, in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/etree.pyx:447\u001b[0m, in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/saxparser.pxi:576\u001b[0m, in \u001b[0;36mlxml.etree._handleSaxData\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/lxml/parsertarget.pxi:108\u001b[0m, in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxData\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\bs4\\builder\\_lxml.py:453\u001b[0m, in \u001b[0;36mLXMLTreeBuilderForXML.data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_data(data)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mendData(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_instruction_class)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# thay vì zoneinfo:\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    # \"https://vnexpress.net/the-gioi\",\n",
    "    # \"https://vnexpress.net/thoi-su\",\n",
    "    # \"https://vnexpress.net/kinh-doanh\",\n",
    "    # \"https://vnexpress.net/khoa-hoc-cong-nghe\",\n",
    "    # \"https://vnexpress.net/goc-nhin\",\n",
    "    # \"https://vnexpress.net/bat-dong-san\",\n",
    "    # \"https://vnexpress.net/suc-khoe\",\n",
    "    # \"https://vnexpress.net/the-thao\",\n",
    "    # \"https://vnexpress.net/giai-tri\",\n",
    "    # \"https://vnexpress.net/phap-luat\",\n",
    "    \"https://vnexpress.net/giao-duc\",\n",
    "    \"https://vnexpress.net/du-lich\",\n",
    "    \"https://vnexpress.net/oto-xe-may\",\n",
    "    \"https://vnexpress.net/doi-song\"\n",
    "]\n",
    "\n",
    "# Crawl từ mới -> cũ cho tới khi bài có ngày < END_DATE (theo giờ VN)\n",
    "END_DATE = \"2026-02-01\"  # YYYY-MM-DD\n",
    "MAX_PAGES_PER_CATEGORY = 200  # safety stop\n",
    "\n",
    "CSV_PATH = \"vnexpress_html_categories_vi_v2.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; VnExpressHTMLCrawler/1.0)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"VnExpress\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def is_english_site(url: str) -> bool:\n",
    "    return bool(url) and (\n",
    "        url.startswith(\"https://e.vnexpress.net/\") or url.startswith(\"http://e.vnexpress.net/\")\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def to_iso_utc(s: Optional[str]) -> Optional[str]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(s)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            # nếu không có tz, coi như giờ VN (thực tế VnExpress thường có tz trong meta)\n",
    "            if VN_TZ:\n",
    "                dt = dt.replace(tzinfo=VN_TZ)\n",
    "            else:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    iso_utc -> YYYY-MM-DD theo giờ VN để so sánh với END_DATE.\n",
    "    \"\"\"\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(iso_utc)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        if VN_TZ:\n",
    "            dt_local = dt.astimezone(VN_TZ)\n",
    "        else:\n",
    "            dt_local = dt\n",
    "        return dt_local.date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def extract_language_from_html(soup: BeautifulSoup) -> str:\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            lang = lang.lower().strip()\n",
    "            if lang.startswith(\"vi\"):\n",
    "                return \"vi\"\n",
    "            if lang.startswith(\"en\"):\n",
    "                return \"en\"\n",
    "            return lang\n",
    "    return DEFAULT_LANGUAGE\n",
    "\n",
    "\n",
    "def extract_keywords_from_html(soup: BeautifulSoup) -> List[str]:\n",
    "    for sel in ['meta[name=\"keywords\"]', 'meta[name=\"news_keywords\"]']:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag and tag.get(\"content\"):\n",
    "            raw = tag[\"content\"]\n",
    "            kws = [x.strip() for x in raw.split(\",\") if x.strip()]\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for k in kws:\n",
    "                if k not in seen:\n",
    "                    seen.add(k)\n",
    "                    out.append(k)\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1.title-detail, h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = to_iso_utc(m_pub[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        m2 = soup.select_one('meta[itemprop=\"datePublished\"]')\n",
    "        if m2 and m2.get(\"content\"):\n",
    "            pub = to_iso_utc(m2[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        ttag = soup.select_one(\"time\")\n",
    "        if ttag:\n",
    "            pub = to_iso_utc(ttag.get(\"datetime\") or ttag.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category.primary (fallback từ meta)\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    language = extract_language_from_html(soup)\n",
    "    keywords = extract_keywords_from_html(soup)\n",
    "\n",
    "    # content.text - trích xuất nội dung bài báo\n",
    "    content_text = \"\"\n",
    "    # VnExpress thường dùng class .fck_detail hoặc .Normal\n",
    "    article_body = soup.select_one(\"article.fck_detail\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".fck_detail\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article.content_detail\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".content_detail\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    \n",
    "    if article_body:\n",
    "        # Lấy tất cả đoạn văn\n",
    "        paragraphs = article_body.find_all(\"p\", class_=\"Normal\")\n",
    "        if not paragraphs:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "        \n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    urls = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://vnexpress.net\" + href\n",
    "        if not href.startswith(\"https://vnexpress.net/\"):\n",
    "            continue\n",
    "        if is_english_site(href):\n",
    "            continue\n",
    "        # bài viết thường kết thúc .html\n",
    "        if \".html\" not in href:\n",
    "            continue\n",
    "        # loại các link tracking/ảnh/video nếu cần\n",
    "        urls.append(href.split(\"?\")[0])\n",
    "\n",
    "    # unique giữ thứ tự\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Cố gắng tìm link trang kế tiếp.\n",
    "    VnExpress có thể dùng ?p=2 hoặc /p2 hoặc cấu trúc khác tùy thời điểm.\n",
    "    Nếu không tìm được thì fallback theo quy ước ?p=...\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # thử rel=next\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        href = ln[\"href\"].strip()\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://vnexpress.net\" + href\n",
    "        return href\n",
    "\n",
    "    # thử anchor có text \"Sau\" / \"Tiếp\" / class pagination\n",
    "    for sel in ['a.next', 'a[rel=\"next\"]', '.pagination a.next', 'a:contains(\"Tiếp\")', 'a:contains(\"Sau\")']:\n",
    "        # BeautifulSoup không hỗ trợ :contains chuẩn, nên chỉ dùng selector đơn giản\n",
    "        pass\n",
    "\n",
    "    # fallback: thử pattern ?p=\n",
    "    # nếu category_url đã có ?p= thì tăng lên\n",
    "    if \"?p=\" in category_url:\n",
    "        return re.sub(r\"\\?p=\\d+\", f\"?p={current_page+1}\", category_url)\n",
    "    return f\"{category_url}?p={current_page+1}\"\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    id_ = md5_id(url)\n",
    "    category_primary = meta.get(\"category_from_article\") or category_fallback\n",
    "    keywords_str = \"|\".join(meta.get(\"keywords\") or [])\n",
    "    entities_str = \"|\".join(meta.get(\"entities\") or [])\n",
    "\n",
    "    return {\n",
    "        \"id\": id_,\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": category_primary or \"\",\n",
    "        \"keywords\": keywords_str,\n",
    "        \"entities\": entities_str,\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> int:\n",
    "    \"\"\"\n",
    "    Crawl 1 chuyên mục từ mới -> cũ đến khi < end_date (YYYY-MM-DD, theo giờ VN).\n",
    "    \"\"\"\n",
    "    added = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "\n",
    "    # dùng slug của category làm fallback category.primary\n",
    "    category_slug = category_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    while page <= MAX_PAGES_PER_CATEGORY and url_page:\n",
    "        html = fetch_text(url_page)\n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if DEBUG:\n",
    "            log(f\"[{category_slug}] page {page} got {len(article_urls)} candidate urls: {url_page}\")\n",
    "\n",
    "        if not article_urls:\n",
    "            # không có bài -> dừng\n",
    "            break\n",
    "\n",
    "        page_has_any_new = False\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            if is_english_site(aurl):\n",
    "                continue\n",
    "\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "\n",
    "            # fetch article\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # nếu có ngày và nhỏ hơn end_date => đánh dấu cũ\n",
    "            if pub_local_date and pub_local_date < end_date:\n",
    "                # bài này cũ hơn end_date\n",
    "                # vẫn coi là old\n",
    "                pass\n",
    "            else:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # Nếu bài >= end_date thì ghi\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "                page_has_any_new = True\n",
    "\n",
    "        # Nếu cả trang toàn bài cũ hơn end_date thì dừng category này\n",
    "        if page_all_older_than_end:\n",
    "            if DEBUG:\n",
    "                log(f\"[{category_slug}] stop: page {page} all older than end_date={end_date}\")\n",
    "            break\n",
    "\n",
    "        # đi trang tiếp\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if next_url == url_page:\n",
    "            break\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"[{cat}] added {added}\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ERROR: {e}\")\n",
    "\n",
    "    print(f\"Done. Total appended {total} rows to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
