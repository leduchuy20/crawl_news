{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4c8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://thanhnien.vn/thoi-su.htm] added 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 515\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Total appended \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 505\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m CATEGORY_URLS:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m         added \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m added\n",
      "Cell \u001b[1;32mIn[7], line 464\u001b[0m, in \u001b[0;36mcrawl_category\u001b[1;34m(category_url, end_date, seen_urls, seen_ids)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[43mpolite_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m pub_iso \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished_at\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m pub_local_date \u001b[38;5;241m=\u001b[39m iso_to_local_date(pub_iso) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 111\u001b[0m, in \u001b[0;36mpolite_sleep\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpolite_sleep\u001b[39m():\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREQUEST_DELAY_BASE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    \"https://thanhnien.vn/thoi-su.htm\",\n",
    "    \"https://thanhnien.vn/the-gioi.htm\",\n",
    "    \"https://thanhnien.vn/doi-song.htm\",\n",
    "    \"https://thanhnien.vn/phap-luat.htm\",\n",
    "    \"https://thanhnien.vn/giao-duc.htm\",\n",
    "    \"https://thanhnien.vn/suc-khoe.htm\",\n",
    "    \"https://thanhnien.vn/kinh-doanh.htm\",\n",
    "    \"https://thanhnien.vn/ban-can-biet.htm\",\n",
    "    \"https://thanhnien.vn/the-thao.htm\",\n",
    "    \"https://thanhnien.vn/giai-tri.htm\",\n",
    "    \"https://thanhnien.vn/xe.htm\",\n",
    "    \"https://thanhnien.vn/cong-nghe.htm\",\n",
    "    \"https://thanhnien.vn/du-lich.htm\",\n",
    "]\n",
    "\n",
    "# Crawl từ mới -> cũ cho tới khi bài có ngày < END_DATE (theo giờ VN)\n",
    "END_DATE = \"2026-02-01\"  # YYYY-MM-DD\n",
    "\n",
    "# Không giới hạn số trang; chỉ dừng theo END_DATE hoặc hết trang\n",
    "MAX_PAGES_PER_CATEGORY = None\n",
    "\n",
    "CSV_PATH = \"thanhnien_html_categories_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; ThanhNienHTMLCrawler/1.0; +https://thanhnien.vn/)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",        # ISO UTC\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"ThanhNien\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# lọc link không phải bài\n",
    "BLOCKED_PREFIXES = (\n",
    "    \"/video/\",\n",
    "    \"/tag/\",\n",
    "    \"/photo/\",\n",
    "    \"/multimedia/\",\n",
    ")\n",
    "\n",
    "BLOCKED_PATTERNS = (\n",
    "    \"tin-nhanh-360.htm\",\n",
    "    \"thong-tin-toa-soan.html\",\n",
    "    \"rss.html\",\n",
    "    \"policy.html\",\n",
    ")\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(iso_utc.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(VN_TZ).date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_time_to_iso_utc(dt_str: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Thanh Niên format: có thể là ISO hoặc format khác\n",
    "    Output: ISO UTC (YYYY-MM-DDTHH:MM:SS+00:00)\n",
    "    \"\"\"\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    dt_str = dt_str.strip()\n",
    "    \n",
    "    # Thử parse ISO format trước\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=VN_TZ)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Thử format DD/MM/YYYY HH:MM\n",
    "    try:\n",
    "        dt_local = datetime.strptime(dt_str, \"%d/%m/%Y %H:%M\").replace(tzinfo=VN_TZ)\n",
    "        return dt_local.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Thử format YYYY-MM-DD HH:MM\n",
    "    try:\n",
    "        dt_local = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M\").replace(tzinfo=VN_TZ)\n",
    "        return dt_local.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    # Thử meta tag article:published_time\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = parse_time_to_iso_utc(m_pub[\"content\"]) or \"\"\n",
    "    \n",
    "    # Thử time tag với datetime\n",
    "    if not pub:\n",
    "        t_tag = soup.select_one(\"time[datetime]\")\n",
    "        if t_tag and t_tag.get(\"datetime\"):\n",
    "            pub = parse_time_to_iso_utc(t_tag[\"datetime\"]) or \"\"\n",
    "    \n",
    "    # Thử tìm trong class date hoặc time\n",
    "    if not pub:\n",
    "        date_elem = soup.select_one(\".time, .date, .cms-date\")\n",
    "        if date_elem:\n",
    "            pub = parse_time_to_iso_utc(date_elem.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category.primary\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    # language (đa số vi)\n",
    "    language = DEFAULT_LANGUAGE\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            language = lang.lower().strip()\n",
    "\n",
    "    # keywords (nếu có)\n",
    "    keywords = []\n",
    "    kw = soup.select_one('meta[name=\"keywords\"]')\n",
    "    if kw and kw.get(\"content\"):\n",
    "        keywords = [x.strip() for x in kw[\"content\"].split(\",\") if x.strip()]\n",
    "\n",
    "    # content.text - trích xuất nội dung bài báo\n",
    "    content_text = \"\"\n",
    "    # Thanh Niên thường dùng class .detail-cmain hoặc .detail-content\n",
    "    article_body = soup.select_one(\"article .detail-cmain\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".detail-cmain\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".detail-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article .content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    \n",
    "    if article_body:\n",
    "        # Lấy tất cả đoạn văn\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_url(u: str) -> Optional[str]:\n",
    "    if not u:\n",
    "        return None\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = \"https://thanhnien.vn\" + u\n",
    "    if not (u.startswith(\"https://thanhnien.vn/\") or u.startswith(\"http://thanhnien.vn/\")):\n",
    "        return None\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        p = p._replace(query=\"\", fragment=\"\")\n",
    "        return urlunparse(p)\n",
    "    except Exception:\n",
    "        return u.split(\"?\")[0].split(\"#\")[0]\n",
    "\n",
    "\n",
    "def is_category_url(u: str) -> bool:\n",
    "    # category: /thoi-su.htm (1 segment)\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = (p.path or \"\").strip(\"/\")\n",
    "        return bool(path) and path.endswith(\".htm\") and path.count(\"/\") == 0 and not re.search(r'\\-\\d+\\.htm$', path)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_allowed_article_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = p.path or \"\"\n",
    "        if not path.startswith(\"/\"):\n",
    "            return False\n",
    "\n",
    "        for pref in BLOCKED_PREFIXES:\n",
    "            if path.startswith(pref):\n",
    "                return False\n",
    "        \n",
    "        # Kiểm tra blocked patterns\n",
    "        for pattern in BLOCKED_PATTERNS:\n",
    "            if pattern in u:\n",
    "                return False\n",
    "\n",
    "        # Thanh Niên dùng pattern: /ten-bai-viet-{id}.htm\n",
    "        # ID là chuỗi số dài ở cuối\n",
    "        if not path.endswith(\".htm\"):\n",
    "            return False\n",
    "\n",
    "        if is_category_url(u):\n",
    "            return False\n",
    "\n",
    "        # Bài viết phải có dạng /slug-{số}.htm hoặc /category/slug-{số}.htm\n",
    "        # Ví dụ: /bat-chu-hui-o-ca-mau-185260202003431028.htm\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        if not re.search(r'\\-\\d+\\.htm$', filename):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urls: List[str] = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        nu = normalize_url(a.get(\"href\", \"\"))\n",
    "        if not nu:\n",
    "            continue\n",
    "        if not is_allowed_article_url(nu):\n",
    "            continue\n",
    "        urls.append(nu)\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def set_page_param(url: str, page: int) -> str:\n",
    "    \"\"\"Thanh Niên dùng format trang-{page}.htm\"\"\"\n",
    "    # Nếu đã có trang-X.htm thì thay thế\n",
    "    if re.search(r\"trang-\\d+\\.htm\", url):\n",
    "        return re.sub(r\"trang-\\d+\\.htm\", f\"trang-{page}.htm\", url)\n",
    "    # Nếu chưa có (trang 1), thêm vào\n",
    "    return url.replace(\".htm\", f\"/trang-{page}.htm\")\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Thử tìm link rel=\"next\"\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        nu = normalize_url(ln[\"href\"].strip())\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    # Thử tìm link pagination\n",
    "    a_next = soup.select_one('a[rel=\"next\"], a.next, .pagination a.next')\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        nu = normalize_url(a_next[\"href\"])\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    # Fallback: trang tiếp theo\n",
    "    return set_page_param(category_url, current_page + 1)\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": md5_id(url),\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": (meta.get(\"category_from_article\") or category_fallback) or \"\",\n",
    "        \"keywords\": \"|\".join(meta.get(\"keywords\") or []),\n",
    "        \"entities\": \"|\".join(meta.get(\"entities\") or []),\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def category_slug_from_url(category_url: str) -> str:\n",
    "    path = urlparse(category_url).path\n",
    "    base = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return re.sub(r\"\\.htm$\", \"\", base)\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> int:\n",
    "    added = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "    category_slug = category_slug_from_url(category_url)\n",
    "\n",
    "    while url_page and (MAX_PAGES_PER_CATEGORY is None or page <= MAX_PAGES_PER_CATEGORY):\n",
    "        try:\n",
    "            html = fetch_text(url_page)\n",
    "        except Exception as e:\n",
    "            log(f\"[WARN] Failed to fetch category page {url_page}: {e}\")\n",
    "            break\n",
    "        \n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if not article_urls:\n",
    "            break\n",
    "\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # Nếu bài không có pub => tránh stop sớm\n",
    "            if not pub_local_date:\n",
    "                page_all_older_than_end = False\n",
    "            elif pub_local_date >= end_date:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # chỉ ghi bài >= END_DATE (hoặc không có pub)\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "\n",
    "        # dừng theo END_DATE\n",
    "        if page_all_older_than_end:\n",
    "            break\n",
    "\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if not next_url or next_url == url_page:\n",
    "            break\n",
    "\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"[{cat}] added {added}\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ERROR: {e}\")\n",
    "\n",
    "    print(f\"Done. Total appended {total} rows to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd70db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "URL: https://thanhnien.vn/thoi-su.htm\n",
      "Content length: 307915\n",
      "\n",
      "Total links found: 264\n",
      "Links with .html: 5\n",
      "\n",
      "First 10 .html links:\n",
      "1. https://my.thanhnien.vn/page/login.html?redirect_url=https://thanhnien.vn/thoi-su.htm\n",
      "2. /thong-tin-toa-soan.html\n",
      "3. https://thanhnien.vn/rss.html\n",
      "4. https://thanhnien.vn/thong-tin-toa-soan.html\n",
      "5. https://thanhnien.vn/policy.html\n",
      "\n",
      "Potential article links: 5\n",
      "1. https://my.thanhnien.vn/page/login.html?redirect_url=https://thanhnien.vn/thoi-su.htm\n",
      "2. https://thanhnien.vn/thong-tin-toa-soan.html\n",
      "3. https://thanhnien.vn/rss.html\n",
      "4. https://thanhnien.vn/thong-tin-toa-soan.html\n",
      "5. https://thanhnien.vn/policy.html\n"
     ]
    }
   ],
   "source": [
    "# TEST: Kiểm tra cấu trúc trang Thanh Niên\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "test_url = \"https://thanhnien.vn/thoi-su.htm\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    r = requests.get(test_url, headers=headers, timeout=15)\n",
    "    print(f\"Status: {r.status_code}\")\n",
    "    print(f\"URL: {r.url}\")\n",
    "    print(f\"Content length: {len(r.text)}\")\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    # Tìm tất cả links\n",
    "    all_links = soup.select(\"a[href]\")\n",
    "    print(f\"\\nTotal links found: {len(all_links)}\")\n",
    "    \n",
    "    # Lọc ra các link có chứa .html\n",
    "    html_links = [a.get(\"href\") for a in all_links if a.get(\"href\") and \".html\" in a.get(\"href\")]\n",
    "    print(f\"Links with .html: {len(html_links)}\")\n",
    "    \n",
    "    # In ra 10 link đầu tiên\n",
    "    print(\"\\nFirst 10 .html links:\")\n",
    "    for i, link in enumerate(html_links[:10], 1):\n",
    "        print(f\"{i}. {link}\")\n",
    "    \n",
    "    # Kiểm tra xem có bài viết nào không\n",
    "    article_links = []\n",
    "    for link in html_links[:20]:\n",
    "        if link.startswith(\"/\"):\n",
    "            full_link = \"https://thanhnien.vn\" + link\n",
    "        else:\n",
    "            full_link = link\n",
    "        \n",
    "        if \"thanhnien.vn\" in full_link and \".html\" in full_link:\n",
    "            article_links.append(full_link)\n",
    "    \n",
    "    print(f\"\\nPotential article links: {len(article_links)}\")\n",
    "    for i, link in enumerate(article_links[:5], 1):\n",
    "        print(f\"{i}. {link}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80930fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total internal links: 232\n",
      "\n",
      "Article URL patterns found:\n",
      "\n",
      "/page/... (1 examples):\n",
      "  - https://my.thanhnien.vn/page/login.html?redirect_url=https://thanhnien.vn/thoi-su.htm\n",
      "\n",
      "\n",
      "Links with numeric IDs:\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: Xem tất cả các link và pattern của chúng\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "test_url = \"https://thanhnien.vn/thoi-su.htm\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    r = requests.get(test_url, headers=headers, timeout=15)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    \n",
    "    all_links = soup.select(\"a[href]\")\n",
    "    \n",
    "    # Phân loại các link\n",
    "    internal_links = []\n",
    "    for a in all_links:\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href or href.startswith(\"#\"):\n",
    "            continue\n",
    "        \n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://thanhnien.vn\" + href\n",
    "        \n",
    "        if \"thanhnien.vn\" in href:\n",
    "            internal_links.append(href)\n",
    "    \n",
    "    print(f\"Total internal links: {len(internal_links)}\")\n",
    "    \n",
    "    # Tìm pattern của bài viết\n",
    "    # Thanh Niên có thể dùng pattern khác\n",
    "    article_patterns = {}\n",
    "    for link in internal_links:\n",
    "        parsed = urlparse(link)\n",
    "        path = parsed.path\n",
    "        \n",
    "        # Bỏ qua trang chủ, category\n",
    "        if path in [\"/\", \"/thoi-su.htm\", \"/the-gioi.htm\"] or path.endswith(\".htm\"):\n",
    "            continue\n",
    "        \n",
    "        # Phân tích pattern\n",
    "        parts = path.strip(\"/\").split(\"/\")\n",
    "        if len(parts) >= 2:\n",
    "            pattern = f\"/{parts[0]}/...\"\n",
    "            if pattern not in article_patterns:\n",
    "                article_patterns[pattern] = []\n",
    "            if len(article_patterns[pattern]) < 3:\n",
    "                article_patterns[pattern].append(link)\n",
    "    \n",
    "    print(\"\\nArticle URL patterns found:\")\n",
    "    for pattern, examples in article_patterns.items():\n",
    "        print(f\"\\n{pattern} ({len(examples)} examples):\")\n",
    "        for ex in examples[:3]:\n",
    "            print(f\"  - {ex}\")\n",
    "    \n",
    "    # Tìm các link có số ID (thường là bài viết)\n",
    "    print(\"\\n\\nLinks with numeric IDs:\")\n",
    "    id_links = []\n",
    "    for link in internal_links[:50]:\n",
    "        if any(char.isdigit() for char in link) and not any(x in link for x in [\".htm\", \"login\", \"page\", \"trang-\"]):\n",
    "            id_links.append(link)\n",
    "    \n",
    "    for i, link in enumerate(id_links[:10], 1):\n",
    "        print(f\"{i}. {link}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6054b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for article URL patterns in HTML...\n",
      "\n",
      "Pattern: https://thanhnien\\.vn/[^\"\\'>\\s]+\\-\\d+\\.htm\n",
      "Found 29 matches\n",
      "  - https://thanhnien.vn/ca-sau-khung-xuat-hien-o-kenh-xang-nang-mau-la-thong-tin-sai-su-that-185251212113345486.htm\n",
      "  - https://thanhnien.vn/tang-giay-khen-nam-sinh-ho-tro-phi-cong-trong-vu-may-bay-roi-o-dak-lak-185260129140610757.htm\n",
      "  - https://thanhnien.vn/bat-chu-hui-o-ca-mau-lua-dao-chiem-doat-gan-900-trieu-dong-185260202003431028.htm\n",
      "  - https://thanhnien.vn/tphcm-nhieu-vu-ngo-doc-thuc-pham-khong-xac-dinh-duoc-can-nguyen-185260202152935604.htm\n",
      "  - https://thanhnien.vn/xuc-dong-gap-lai-ong-noi-trong-khu-luu-giu-thi-hai-o-tphcm-1852601302318233.htm\n",
      "\n",
      "Pattern: /[^\"\\'>\\s]+\\-\\d+\\.htm\n",
      "Found 258 matches\n",
      "  - /nguoi-dan-khong-nen-hoang-mang-185260131200624933.htm\n",
      "  - /xuc-dong-gap-lai-ong-noi-trong-khu-luu-giu-thi-hai-o-tphcm-1852601302318233.htm\n",
      "  - /khanh-hoa-khanh-thanh-tru-so-hanh-chinh-tap-trung-hon-544-ti-dong-185260128123250169.htm\n",
      "  - /xa-cu-chi-phoi-hop-cac-don-vi-giao-duc-trao-qua-tet-cho-ho-kho-khan-185260202124552423.htm\n",
      "  - /tu-van-suc-khoe-nhan-dien-cac-benh-phu-khoa-anh-huong-chat-luong-song-phu-nu-185260128163747521.htm\n",
      "\n",
      "Pattern: \"url\":\"([^\"]+)\"\n",
      "Found 1 matches\n",
      "  - https://thanhnien.vn/thoi-su.htm\n",
      "\n",
      "\n",
      "Found 40 script tags\n",
      "\n",
      "--- Script 6 (first 500 chars) ---\n",
      "\n",
      "{\n",
      "    \"@context\": \"http://schema.org\",\n",
      "    \"@type\": \"Organization\",\n",
      "        \"name\":\"Báo Thanh Niên\",\n",
      "        \"url\": \"https://thanhnien.vn/\",\n",
      "         \"logo\": \"https://static.thanhnien.com.vn/thanhnien.vn/image/logo-share.jpg\",\n",
      "        \"email\": \"mailto: toasoan@thanhnien.vn\",\n",
      "        \"sameAs\":[\n",
      "                \"https://www.facebook.com/thanhnien/\"\n",
      "                    ,\"https://www.youtube.com/channel/UCIW9cGgoRuGJnky3K3tbzNg\"\n",
      "                ],\n",
      "    \"contactPoint\": [{\n",
      "    \"@type\": \"ContactPoint\",\n"
     ]
    }
   ],
   "source": [
    "# TEST 3: Xem HTML thô và tìm dữ liệu JSON/API\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "test_url = \"https://thanhnien.vn/thoi-su.htm\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    r = requests.get(test_url, headers=headers, timeout=15)\n",
    "    html = r.text\n",
    "    \n",
    "    # Tìm các link bài viết trong HTML (có thể có pattern khác)\n",
    "    # Thanh Niên có thể dùng /-12345.htm hoặc /slug-12345.htm\n",
    "    patterns_to_check = [\n",
    "        r'https://thanhnien\\.vn/[^\"\\'>\\s]+\\-\\d+\\.htm',  # /slug-123456.htm\n",
    "        r'/[^\"\\'>\\s]+\\-\\d+\\.htm',  # /slug-123456.htm\n",
    "        r'\"url\":\"([^\"]+)\"',  # JSON data\n",
    "    ]\n",
    "    \n",
    "    print(\"Searching for article URL patterns in HTML...\")\n",
    "    for pattern in patterns_to_check:\n",
    "        matches = re.findall(pattern, html)\n",
    "        if matches:\n",
    "            print(f\"\\nPattern: {pattern}\")\n",
    "            print(f\"Found {len(matches)} matches\")\n",
    "            unique_matches = list(set(matches))[:5]\n",
    "            for match in unique_matches:\n",
    "                print(f\"  - {match}\")\n",
    "    \n",
    "    # Tìm script tags có thể chứa dữ liệu\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    scripts = soup.find_all(\"script\")\n",
    "    \n",
    "    print(f\"\\n\\nFound {len(scripts)} script tags\")\n",
    "    \n",
    "    # Tìm các script có JSON data\n",
    "    for i, script in enumerate(scripts):\n",
    "        content = script.string or \"\"\n",
    "        if \"article\" in content.lower() or \"post\" in content.lower():\n",
    "            if len(content) > 100 and len(content) < 5000:\n",
    "                print(f\"\\n--- Script {i+1} (first 500 chars) ---\")\n",
    "                print(content[:500])\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58a63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69 article URLs\n",
      "\n",
      "First 10 articles:\n",
      "1. https://thanhnien.vn/tin-nhanh-360.htm\n",
      "2. https://thanhnien.vn/tham-my-chui-bua-vay-den-tan-nha-khach-de-hanh-nghe-185260201201830897.htm\n",
      "3. https://thanhnien.vn/tuyen-an-100-bi-cao-vu-cap-220000-phieu-thu-nghiem-gia-185260202110100969.htm\n",
      "4. https://thanhnien.vn/tphcm-thanh-lap-37-ban-quan-ly-du-an-truc-thuoc-phuong-xa-1852602020857329.htm\n",
      "5. https://thanhnien.vn/gia-lai-nguoi-tong-xe-vao-csgt-bi-khoi-to-toi-giet-nguoi-185260202102947406.htm\n",
      "6. https://thanhnien.vn/pho-di-bo-ben-bo-bien-dep-nhat-quang-ngai-185260202080808302.htm\n",
      "7. https://thanhnien.vn/dai-tuong-luong-tam-quang-ra-soat-danh-sach-cu-tri-bao-dam-quyen-bau-cu-185260202164814066.htm\n",
      "8. https://thanhnien.vn/tphcm-ra-soat-cong-tac-chuan-bi-bau-cu-185260127171552695.htm\n",
      "9. https://thanhnien.vn/vi-sao-phai-dua-gau-ngua-misa-nang-120-kg-ve-vuon-quoc-gia-bach-ma-185260202153331996.htm\n",
      "10. https://thanhnien.vn/vuon-quoc-gia-chu-mom-ray-gau-ngua-ga-tien-mat-do-quy-hiem-sap-bay-anh-185251009185316022.htm\n"
     ]
    }
   ],
   "source": [
    "# TEST 4: Test lại với code đã sửa\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import re\n",
    "\n",
    "def normalize_url(u: str):\n",
    "    if not u:\n",
    "        return None\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = \"https://thanhnien.vn\" + u\n",
    "    if not (u.startswith(\"https://thanhnien.vn/\") or u.startswith(\"http://thanhnien.vn/\")):\n",
    "        return None\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        p = p._replace(query=\"\", fragment=\"\")\n",
    "        return urlunparse(p)\n",
    "    except Exception:\n",
    "        return u.split(\"?\")[0].split(\"#\")[0]\n",
    "\n",
    "def is_category_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = (p.path or \"\").strip(\"/\")\n",
    "        return bool(path) and path.endswith(\".htm\") and path.count(\"/\") == 0 and not re.search(r'\\-\\d+\\.htm$', path)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_allowed_article_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = p.path or \"\"\n",
    "        if not path.startswith(\"/\"):\n",
    "            return False\n",
    "        \n",
    "        blocked = (\"/video/\", \"/tag/\", \"/photo/\", \"/multimedia/\")\n",
    "        for pref in blocked:\n",
    "            if path.startswith(pref):\n",
    "                return False\n",
    "        \n",
    "        if not path.endswith(\".htm\"):\n",
    "            return False\n",
    "        \n",
    "        if is_category_url(u):\n",
    "            return False\n",
    "        \n",
    "        filename = path.split(\"/\")[-1]\n",
    "        if not re.search(r'\\-\\d+\\.htm$', filename):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Test\n",
    "test_url = \"https://thanhnien.vn/thoi-su.htm\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "r = requests.get(test_url, headers=headers, timeout=15)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "urls = []\n",
    "for a in soup.select(\"a[href]\"):\n",
    "    nu = normalize_url(a.get(\"href\", \"\"))\n",
    "    if not nu:\n",
    "        continue\n",
    "    if not is_allowed_article_url(nu):\n",
    "        continue\n",
    "    urls.append(nu)\n",
    "\n",
    "seen = set()\n",
    "out = []\n",
    "for u in urls:\n",
    "    if u not in seen:\n",
    "        seen.add(u)\n",
    "        out.append(u)\n",
    "\n",
    "print(f\"Found {len(out)} article URLs\")\n",
    "print(\"\\nFirst 10 articles:\")\n",
    "for i, url in enumerate(out[:10], 1):\n",
    "    print(f\"{i}. {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26da5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Thẩm mỹ 'chui' bủa vây: Đến tận nhà khách để hành nghề\n",
      "Published: 2026-02-02T00:22:00+00:00\n",
      "Content length: 7031 chars\n",
      "\n",
      "First 500 chars of content:\n",
      "Không đăng ký kinh doanh, không có giấy phép hoạt động khám chữa bệnh, không có giấy phép hành nghề chuyên môn, nhưng một số người vẫn ngang nhiên hành nghềthẩm mỹ. Hầu hết đều quảng cáo bản thân là bác sĩ, mời chào làm đẹp trên các trang mạng xã hội hay qua hội nhóm, thực hiện các thủ thuật từtiêm fillerđến nâng mũi, cắt mí, độn cằm. Hộp thuốc mà cô gái tên L. dự định tiêm cho PV Ảnh: Thanh Niên Chỉ cần một tin nhắn hay cuộc điện thoại hẹn, họ sẽ đến nhà khách hàng cùng chiếc vali hoặc túi xách\n"
     ]
    }
   ],
   "source": [
    "# TEST 5: Test crawl một bài viết thực tế\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "def parse_time_to_iso_utc(dt_str: str):\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    dt_str = dt_str.strip()\n",
    "    \n",
    "    # Thử parse ISO format trước\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=VN_TZ)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test với một bài viết thực\n",
    "test_article = \"https://thanhnien.vn/tham-my-chui-bua-vay-den-tan-nha-khach-de-hanh-nghe-185260201201830897.htm\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "r = requests.get(test_article, headers=headers, timeout=15)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# Lấy title\n",
    "title = \"\"\n",
    "og = soup.select_one('meta[property=\"og:title\"]')\n",
    "if og and og.get(\"content\"):\n",
    "    title = og[\"content\"].strip()\n",
    "\n",
    "# Lấy published_at\n",
    "pub = \"\"\n",
    "m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "if m_pub and m_pub.get(\"content\"):\n",
    "    pub = parse_time_to_iso_utc(m_pub[\"content\"]) or \"\"\n",
    "\n",
    "# Lấy content\n",
    "content_text = \"\"\n",
    "article_body = soup.select_one(\"article .detail-cmain\")\n",
    "if not article_body:\n",
    "    article_body = soup.select_one(\".detail-cmain\")\n",
    "if not article_body:\n",
    "    article_body = soup.select_one(\".detail-content\")\n",
    "\n",
    "if article_body:\n",
    "    paragraphs = article_body.find_all(\"p\")\n",
    "    text_parts = []\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if text:\n",
    "            text_parts.append(text)\n",
    "    content_text = \" \".join(text_parts)\n",
    "\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Published: {pub}\")\n",
    "print(f\"Content length: {len(content_text)} chars\")\n",
    "print(f\"\\nFirst 500 chars of content:\\n{content_text[:500]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
