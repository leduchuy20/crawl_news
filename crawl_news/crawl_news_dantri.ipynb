{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:41:20.940133Z",
     "start_time": "2025-12-21T12:41:17.122714Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\work\\big_data\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\work\\big_data\\.venv\\lib\\site-packages (4.14.3)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dateutil in d:\\work\\big_data\\.venv\\lib\\site-packages (2.9.0.post0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\big_data\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached lxml-6.0.2-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, lxml\n",
      "\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   ---------------------------------------- 2/2 [lxml]\n",
      "\n",
      "Successfully installed lxml-6.0.2 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff100c040820690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:45:32.479847Z",
     "start_time": "2025-12-21T12:42:07.690968Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 486\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Total appended \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 486\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 476\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m CATEGORY_URLS:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m         added \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    478\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m added\n",
      "Cell \u001b[1;32mIn[2], line 435\u001b[0m, in \u001b[0;36mcrawl_category\u001b[1;34m(category_url, end_date, seen_urls, seen_ids)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[43mpolite_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m pub_iso \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished_at\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m pub_local_date \u001b[38;5;241m=\u001b[39m iso_to_local_date(pub_iso) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 119\u001b[0m, in \u001b[0;36mpolite_sleep\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpolite_sleep\u001b[39m():\n\u001b[1;32m--> 119\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREQUEST_DELAY_BASE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    # \"https://dantri.com.vn/the-gioi.htm\",\n",
    "    # \"https://dantri.com.vn/thoi-su.htm\",\n",
    "    # \"https://dantri.com.vn/phap-luat.htm\",\n",
    "    \"https://dantri.com.vn/suc-khoe.htm\",\n",
    "    \"https://dantri.com.vn/doi-song.htm\",\n",
    "    \"https://dantri.com.vn/du-lich.htm\",\n",
    "    \"https://dantri.com.vn/kinh-doanh.htm\",\n",
    "    \"https://dantri.com.vn/the-thao.htm\",\n",
    "    \"https://dantri.com.vn/giai-tri.htm\",\n",
    "    \"https://dantri.com.vn/giao-duc.htm\",\n",
    "    \"https://dantri.com.vn/cong-nghe.htm\",\n",
    "]\n",
    "\n",
    "# Crawl từ mới -> cũ cho tới khi bài có ngày < END_DATE (theo giờ VN)\n",
    "END_DATE = \"2026-02-01\"  # YYYY-MM-DD\n",
    "\n",
    "# Không giới hạn số trang; chỉ dừng theo END_DATE hoặc hết trang\n",
    "MAX_PAGES_PER_CATEGORY = None\n",
    "\n",
    "CSV_PATH = \"dantri_html_categories_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DanTriHTMLCrawler/2.0; +https://dantri.com.vn/)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",        # ISO UTC\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"DanTri\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# lọc link không phải bài\n",
    "BLOCKED_PREFIXES = (\n",
    "    \"/event/\",\n",
    "    \"/tag/\",\n",
    "    \"/video/\",\n",
    "    \"/photo/\",\n",
    "    \"/infographic/\",\n",
    "    \"/emagazine/\",\n",
    "    \"/interactive/\",\n",
    ")\n",
    "\n",
    "ALLOWED_SECTIONS = {\n",
    "    \"the-gioi\",\n",
    "    \"thoi-su\",\n",
    "    \"phap-luat\",\n",
    "    \"suc-khoe\",\n",
    "    \"doi-song\",\n",
    "    \"du-lich\",\n",
    "    \"kinh-doanh\",\n",
    "    \"the-thao\",\n",
    "    \"giai-tri\",\n",
    "    \"giao-duc\",\n",
    "    \"cong-nghe\",\n",
    "}\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# def fetch_text(url: str) -> str:\n",
    "#     r = session.get(url, timeout=TIMEOUT)\n",
    "#     r.raise_for_status()\n",
    "#     return r.text\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT, allow_redirects=True)\n",
    "    # print(\"FETCH:\", url)\n",
    "    # print(\"STATUS:\", r.status_code)\n",
    "    # print(\"FINAL_URL:\", r.url)\n",
    "    # print(\"CONTENT_TYPE:\", r.headers.get(\"Content-Type\", \"\"))\n",
    "    # print(\"LEN:\", len(r.text))\n",
    "    # print(\"HAS author-time:\", \"author-time\" in r.text)\n",
    "    # # in thử 500 ký tự đầu\n",
    "    # print(\"HEAD_SNIP:\", r.text[:500].replace(\"\\n\", \" \")[:500])\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(iso_utc.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(VN_TZ).date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_author_time_to_iso_utc(dt_str: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Dantri: datetime=\"YYYY-MM-DD HH:MM\" (giờ VN)\n",
    "    Output: ISO UTC (YYYY-MM-DDTHH:MM:SS+00:00)\n",
    "    \"\"\"\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    dt_str = dt_str.strip()\n",
    "    try:\n",
    "        dt_local = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M\").replace(tzinfo=VN_TZ)\n",
    "        return dt_local.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        # phòng khi có giây\n",
    "        try:\n",
    "            dt_local = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=VN_TZ)\n",
    "            return dt_local.astimezone(timezone.utc).isoformat()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at (chuẩn Dantri theo bạn cung cấp)\n",
    "    pub = \"\"\n",
    "    t_auth = soup.select_one(\"time.author-time[datetime]\")\n",
    "    if t_auth and t_auth.get(\"datetime\"):\n",
    "        pub = parse_author_time_to_iso_utc(t_auth[\"datetime\"]) or \"\"\n",
    "\n",
    "    # category.primary\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    # language (đa số vi)\n",
    "    language = DEFAULT_LANGUAGE\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            language = lang.lower().strip()\n",
    "\n",
    "    # keywords (nếu có)\n",
    "    keywords = []\n",
    "    kw = soup.select_one('meta[name=\"keywords\"]')\n",
    "    if kw and kw.get(\"content\"):\n",
    "        keywords = [x.strip() for x in kw[\"content\"].split(\",\") if x.strip()]\n",
    "\n",
    "    # content.text - trích xuất nội dung bài báo\n",
    "    content_text = \"\"\n",
    "    # Thử các selector phổ biến của Dantri\n",
    "    article_body = soup.select_one(\"article .detail-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".detail-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".article-content\")\n",
    "    \n",
    "    if article_body:\n",
    "        # Lấy tất cả đoạn văn\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_url(u: str) -> Optional[str]:\n",
    "    if not u:\n",
    "        return None\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = \"https://dantri.com.vn\" + u\n",
    "    if not (u.startswith(\"https://dantri.com.vn/\") or u.startswith(\"http://dantri.com.vn/\")):\n",
    "        return None\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        p = p._replace(query=\"\", fragment=\"\")\n",
    "        return urlunparse(p)\n",
    "    except Exception:\n",
    "        return u.split(\"?\")[0].split(\"#\")[0]\n",
    "\n",
    "\n",
    "def is_category_url(u: str) -> bool:\n",
    "    # category: /the-gioi.htm (1 segment)\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = (p.path or \"\").strip(\"/\")\n",
    "        return bool(path) and path.endswith(\".htm\") and path.count(\"/\") == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_allowed_article_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = p.path or \"\"\n",
    "        if not path.startswith(\"/\"):\n",
    "            return False\n",
    "\n",
    "        for pref in BLOCKED_PREFIXES:\n",
    "            if path.startswith(pref):\n",
    "                return False\n",
    "\n",
    "        if not path.endswith(\".htm\"):\n",
    "            return False\n",
    "\n",
    "        if is_category_url(u):\n",
    "            return False\n",
    "\n",
    "        parts = [x for x in path.strip(\"/\").split(\"/\") if x]\n",
    "        if len(parts) < 2:\n",
    "            return False\n",
    "\n",
    "        return parts[0] in ALLOWED_SECTIONS\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urls: List[str] = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        nu = normalize_url(a.get(\"href\", \"\"))\n",
    "        if not nu:\n",
    "            continue\n",
    "        if not is_allowed_article_url(nu):\n",
    "            continue\n",
    "        urls.append(nu)\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def set_page_param(url: str, page: int) -> str:\n",
    "    p = urlparse(url)\n",
    "    qs = parse_qs(p.query, keep_blank_values=True)\n",
    "    qs[\"page\"] = [str(page)]\n",
    "    return urlunparse(p._replace(query=urlencode(qs, doseq=True)))\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        nu = normalize_url(ln[\"href\"].strip())\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    a_next = soup.select_one('a[rel=\"next\"], a.next, .pagination a.next')\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        nu = normalize_url(a_next[\"href\"])\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    return set_page_param(category_url, current_page + 1)\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": md5_id(url),\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": (meta.get(\"category_from_article\") or category_fallback) or \"\",\n",
    "        \"keywords\": \"|\".join(meta.get(\"keywords\") or []),\n",
    "        \"entities\": \"|\".join(meta.get(\"entities\") or []),\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def category_slug_from_url(category_url: str) -> str:\n",
    "    path = urlparse(category_url).path\n",
    "    base = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return re.sub(r\"\\.htm$\", \"\", base)\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> int:\n",
    "    added = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "    category_slug = category_slug_from_url(category_url)\n",
    "\n",
    "    while url_page and (MAX_PAGES_PER_CATEGORY is None or page <= MAX_PAGES_PER_CATEGORY):\n",
    "        html = fetch_text(url_page)\n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if not article_urls:\n",
    "            break\n",
    "\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # Nếu bài không có pub => tránh stop sớm (nhưng theo bạn thì sẽ có)\n",
    "            if not pub_local_date:\n",
    "                page_all_older_than_end = False\n",
    "            elif pub_local_date >= end_date:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # chỉ ghi bài >= END_DATE (hoặc không có pub)\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "\n",
    "        # dừng theo END_DATE\n",
    "        if page_all_older_than_end:\n",
    "            break\n",
    "\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if not next_url or next_url == url_page:\n",
    "            break\n",
    "\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"[{cat}] added {added}\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ERROR: {e}\")\n",
    "\n",
    "    print(f\"Done. Total appended {total} rows to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
