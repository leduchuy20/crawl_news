{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:41:20.940133Z",
     "start_time": "2025-12-21T12:41:17.122714Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\work\\big_data\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\work\\big_data\\.venv\\lib\\site-packages (4.14.3)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dateutil in d:\\work\\big_data\\.venv\\lib\\site-packages (2.9.0.post0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\big_data\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached lxml-6.0.2-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, lxml\n",
      "\n",
      "   ---------------------------------------- 0/2 [tqdm]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [lxml]\n",
      "   ---------------------------------------- 2/2 [lxml]\n",
      "\n",
      "Successfully installed lxml-6.0.2 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff100c040820690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T12:45:32.479847Z",
     "start_time": "2025-12-21T12:42:07.690968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[https://dantri.com.vn/suc-khoe.htm] added 687\n",
      "[https://dantri.com.vn/doi-song.htm] added 443\n",
      "[https://dantri.com.vn/du-lich.htm] added 384\n",
      "[https://dantri.com.vn/kinh-doanh.htm] added 746\n",
      "[https://dantri.com.vn/the-thao.htm] added 718\n",
      "[https://dantri.com.vn/giai-tri.htm] added 753\n",
      "[https://dantri.com.vn/giao-duc.htm] added 611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 462\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Total appended \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 462\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 452\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m CATEGORY_URLS:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m         added \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    454\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m added\n",
      "Cell \u001b[1;32mIn[1], line 405\u001b[0m, in \u001b[0;36mcrawl_category\u001b[1;34m(category_url, end_date, seen_urls, seen_ids)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     ah \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43maurl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m     meta \u001b[38;5;241m=\u001b[39m extract_article_meta(ah)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m, in \u001b[0;36mfetch_text\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_text\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# print(\"FETCH:\", url)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# print(\"STATUS:\", r.status_code)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# print(\"FINAL_URL:\", r.url)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# # in thử 500 ký tự đầu\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# print(\"HEAD_SNIP:\", r.text[:500].replace(\"\\n\", \" \")[:500])\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32md:\\work\\big_data\\.venv\\lib\\site-packages\\urllib3\\connection.py:571\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 571\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    # \"https://dantri.com.vn/the-gioi.htm\",\n",
    "    # \"https://dantri.com.vn/thoi-su.htm\",\n",
    "    # \"https://dantri.com.vn/phap-luat.htm\",\n",
    "    \"https://dantri.com.vn/suc-khoe.htm\",\n",
    "    \"https://dantri.com.vn/doi-song.htm\",\n",
    "    \"https://dantri.com.vn/du-lich.htm\",\n",
    "    \"https://dantri.com.vn/kinh-doanh.htm\",\n",
    "    \"https://dantri.com.vn/the-thao.htm\",\n",
    "    \"https://dantri.com.vn/giai-tri.htm\",\n",
    "    \"https://dantri.com.vn/giao-duc.htm\",\n",
    "    \"https://dantri.com.vn/cong-nghe.htm\",\n",
    "]\n",
    "\n",
    "# Crawl từ mới -> cũ cho tới khi bài có ngày < END_DATE (theo giờ VN)\n",
    "END_DATE = \"2025-11-01\"  # YYYY-MM-DD\n",
    "\n",
    "# Không giới hạn số trang; chỉ dừng theo END_DATE hoặc hết trang\n",
    "MAX_PAGES_PER_CATEGORY = None\n",
    "\n",
    "CSV_PATH = \"dantri_html_categories_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DanTriHTMLCrawler/2.0; +https://dantri.com.vn/)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",        # ISO UTC\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"DanTri\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# lọc link không phải bài\n",
    "BLOCKED_PREFIXES = (\n",
    "    \"/event/\",\n",
    "    \"/tag/\",\n",
    "    \"/video/\",\n",
    "    \"/photo/\",\n",
    "    \"/infographic/\",\n",
    "    \"/emagazine/\",\n",
    "    \"/interactive/\",\n",
    ")\n",
    "\n",
    "ALLOWED_SECTIONS = {\n",
    "    \"the-gioi\",\n",
    "    \"thoi-su\",\n",
    "    \"phap-luat\",\n",
    "    \"suc-khoe\",\n",
    "    \"doi-song\",\n",
    "    \"du-lich\",\n",
    "    \"kinh-doanh\",\n",
    "    \"the-thao\",\n",
    "    \"giai-tri\",\n",
    "    \"giao-duc\",\n",
    "    \"cong-nghe\",\n",
    "}\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# def fetch_text(url: str) -> str:\n",
    "#     r = session.get(url, timeout=TIMEOUT)\n",
    "#     r.raise_for_status()\n",
    "#     return r.text\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT, allow_redirects=True)\n",
    "    # print(\"FETCH:\", url)\n",
    "    # print(\"STATUS:\", r.status_code)\n",
    "    # print(\"FINAL_URL:\", r.url)\n",
    "    # print(\"CONTENT_TYPE:\", r.headers.get(\"Content-Type\", \"\"))\n",
    "    # print(\"LEN:\", len(r.text))\n",
    "    # print(\"HAS author-time:\", \"author-time\" in r.text)\n",
    "    # # in thử 500 ký tự đầu\n",
    "    # print(\"HEAD_SNIP:\", r.text[:500].replace(\"\\n\", \" \")[:500])\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(iso_utc.replace(\"Z\", \"+00:00\"))\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(VN_TZ).date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_author_time_to_iso_utc(dt_str: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Dantri: datetime=\"YYYY-MM-DD HH:MM\" (giờ VN)\n",
    "    Output: ISO UTC (YYYY-MM-DDTHH:MM:SS+00:00)\n",
    "    \"\"\"\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    dt_str = dt_str.strip()\n",
    "    try:\n",
    "        dt_local = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M\").replace(tzinfo=VN_TZ)\n",
    "        return dt_local.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        # phòng khi có giây\n",
    "        try:\n",
    "            dt_local = datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=VN_TZ)\n",
    "            return dt_local.astimezone(timezone.utc).isoformat()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at (chuẩn Dantri theo bạn cung cấp)\n",
    "    pub = \"\"\n",
    "    t_auth = soup.select_one(\"time.author-time[datetime]\")\n",
    "    if t_auth and t_auth.get(\"datetime\"):\n",
    "        pub = parse_author_time_to_iso_utc(t_auth[\"datetime\"]) or \"\"\n",
    "\n",
    "    # category.primary\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    # language (đa số vi)\n",
    "    language = DEFAULT_LANGUAGE\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            language = lang.lower().strip()\n",
    "\n",
    "    # keywords (nếu có)\n",
    "    keywords = []\n",
    "    kw = soup.select_one('meta[name=\"keywords\"]')\n",
    "    if kw and kw.get(\"content\"):\n",
    "        keywords = [x.strip() for x in kw[\"content\"].split(\",\") if x.strip()]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def normalize_url(u: str) -> Optional[str]:\n",
    "    if not u:\n",
    "        return None\n",
    "    u = u.strip()\n",
    "    if u.startswith(\"//\"):\n",
    "        u = \"https:\" + u\n",
    "    if u.startswith(\"/\"):\n",
    "        u = \"https://dantri.com.vn\" + u\n",
    "    if not (u.startswith(\"https://dantri.com.vn/\") or u.startswith(\"http://dantri.com.vn/\")):\n",
    "        return None\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        p = p._replace(query=\"\", fragment=\"\")\n",
    "        return urlunparse(p)\n",
    "    except Exception:\n",
    "        return u.split(\"?\")[0].split(\"#\")[0]\n",
    "\n",
    "\n",
    "def is_category_url(u: str) -> bool:\n",
    "    # category: /the-gioi.htm (1 segment)\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = (p.path or \"\").strip(\"/\")\n",
    "        return bool(path) and path.endswith(\".htm\") and path.count(\"/\") == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_allowed_article_url(u: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        path = p.path or \"\"\n",
    "        if not path.startswith(\"/\"):\n",
    "            return False\n",
    "\n",
    "        for pref in BLOCKED_PREFIXES:\n",
    "            if path.startswith(pref):\n",
    "                return False\n",
    "\n",
    "        if not path.endswith(\".htm\"):\n",
    "            return False\n",
    "\n",
    "        if is_category_url(u):\n",
    "            return False\n",
    "\n",
    "        parts = [x for x in path.strip(\"/\").split(\"/\") if x]\n",
    "        if len(parts) < 2:\n",
    "            return False\n",
    "\n",
    "        return parts[0] in ALLOWED_SECTIONS\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urls: List[str] = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        nu = normalize_url(a.get(\"href\", \"\"))\n",
    "        if not nu:\n",
    "            continue\n",
    "        if not is_allowed_article_url(nu):\n",
    "            continue\n",
    "        urls.append(nu)\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def set_page_param(url: str, page: int) -> str:\n",
    "    p = urlparse(url)\n",
    "    qs = parse_qs(p.query, keep_blank_values=True)\n",
    "    qs[\"page\"] = [str(page)]\n",
    "    return urlunparse(p._replace(query=urlencode(qs, doseq=True)))\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        nu = normalize_url(ln[\"href\"].strip())\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    a_next = soup.select_one('a[rel=\"next\"], a.next, .pagination a.next')\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        nu = normalize_url(a_next[\"href\"])\n",
    "        if nu:\n",
    "            return nu\n",
    "\n",
    "    return set_page_param(category_url, current_page + 1)\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": md5_id(url),\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": (meta.get(\"category_from_article\") or category_fallback) or \"\",\n",
    "        \"keywords\": \"|\".join(meta.get(\"keywords\") or []),\n",
    "        \"entities\": \"|\".join(meta.get(\"entities\") or []),\n",
    "    }\n",
    "\n",
    "\n",
    "def category_slug_from_url(category_url: str) -> str:\n",
    "    path = urlparse(category_url).path\n",
    "    base = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return re.sub(r\"\\.htm$\", \"\", base)\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> int:\n",
    "    added = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "    category_slug = category_slug_from_url(category_url)\n",
    "\n",
    "    while url_page and (MAX_PAGES_PER_CATEGORY is None or page <= MAX_PAGES_PER_CATEGORY):\n",
    "        html = fetch_text(url_page)\n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if not article_urls:\n",
    "            break\n",
    "\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # Nếu bài không có pub => tránh stop sớm (nhưng theo bạn thì sẽ có)\n",
    "            if not pub_local_date:\n",
    "                page_all_older_than_end = False\n",
    "            elif pub_local_date >= end_date:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # chỉ ghi bài >= END_DATE (hoặc không có pub)\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "\n",
    "        # dừng theo END_DATE\n",
    "        if page_all_older_than_end:\n",
    "            break\n",
    "\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if not next_url or next_url == url_page:\n",
    "            break\n",
    "\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"[{cat}] added {added}\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ERROR: {e}\")\n",
    "\n",
    "    print(f\"Done. Total appended {total} rows to {CSV_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
