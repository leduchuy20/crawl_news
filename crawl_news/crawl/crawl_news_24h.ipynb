{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194865d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "24H.COM.VN CRAWLER - Duplicate-Safe Daily Crawling\n",
      "================================================================================\n",
      "\n",
      "üìä Initial state:\n",
      "  - Already crawled: 358 URLs, 358 IDs\n",
      "  - Date filter: Articles >= 2026-01-30\n",
      "  - Total feeds: 11\n",
      "\n",
      "  [trang-chu] RSS entries: 108 | Added: 0 | Duplicates: 108 | Old: 0\n",
      "  [tin-tuc-trong-ngay] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [bong-da] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [the-thao] RSS entries: 1 | Added: 0 | Duplicates: 0 | Old: 1\n",
      "  [thoi-trang] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [hi-tech] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [tai-chinh-bat-dong-san] RSS entries: 24 | Added: 0 | Duplicates: 0 | Old: 24\n",
      "  [phim] RSS entries: 24 | Added: 0 | Duplicates: 9 | Old: 15\n",
      "  [giao-duc-du-hoc] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [ban-tre-cuoc-song] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "  [the-thao] RSS entries: 24 | Added: 0 | Duplicates: 24 | Old: 0\n",
      "\n",
      "================================================================================\n",
      "‚úÖ CRAWL SUMMARY\n",
      "================================================================================\n",
      "üìù New articles added: 0\n",
      "üîÅ Duplicates skipped: 285 (already in CSV)\n",
      "‚è∞ Old articles skipped: 40 (before 2026-01-30)\n",
      "üìä Total processed: 325\n",
      "üíæ Output: 24h_html_categories_vi.csv\n",
      "üìà Total in CSV now: 358 articles\n",
      "================================================================================\n",
      "\n",
      "üí° Duplicate rate: 87.7% - Perfect for daily runs!\n",
      "   (High rate = most articles already crawled = efficient)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "RSS_FEEDS = [\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/trangchu24h.rss\", \"trang-chu\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/tintuctrongngay.rss\", \"tin-tuc-trong-ngay\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/bongda.rss\", \"bong-da\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/asiancup2019.rss\", \"the-thao\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/thoitrang.rss\", \"thoi-trang\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/thoitranghitech.rss\", \"hi-tech\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/taichinhbatdongsan.rss\", \"tai-chinh-bat-dong-san\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/phim.rss\", \"phim\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/giaoducduhoc.rss\", \"giao-duc-du-hoc\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/bantrecuocsong.rss\", \"ban-tre-cuoc-song\"),\n",
    "    (\"https://cdn.24h.com.vn/upload/rss/thethao.rss\", \"the-thao\"),\n",
    "]\n",
    "\n",
    "# Crawl t·ª´ m·ªõi -> c≈© cho t·ªõi khi b√†i c√≥ ng√†y < END_DATE (theo gi·ªù VN)\n",
    "# L∆∞u √Ω: RSS c·ªßa 24h ch·ªâ cung c·∫•p ~5 ng√†y data g·∫ßn nh·∫•t\n",
    "END_DATE = \"2026-01-30\"  # YYYY-MM-DD (ƒëi·ªÅu ch·ªânh ph√π h·ª£p v·ªõi RSS limitation)\n",
    "\n",
    "CSV_PATH = \"24h_html_categories_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; 24hHTMLCrawler/1.0)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"24h\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    # X·ª≠ l√Ω encoding ƒë·∫∑c bi·ªát c·ªßa 24h\n",
    "    r.encoding = r.apparent_encoding or 'utf-8'\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def fetch_rss(rss_url: str) -> feedparser.FeedParserDict:\n",
    "    \"\"\"Fetch v√† parse RSS feed, x·ª≠ l√Ω encoding ƒë√∫ng c√°ch\"\"\"\n",
    "    r = session.get(rss_url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    # Feedparser t·ª± x·ª≠ l√Ω encoding\n",
    "    feed = feedparser.parse(r.content)\n",
    "    return feed\n",
    "\n",
    "\n",
    "def to_iso_utc(s: Optional[str]) -> Optional[str]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(s)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            if VN_TZ:\n",
    "                dt = dt.replace(tzinfo=VN_TZ)\n",
    "            else:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(iso_utc)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        if VN_TZ:\n",
    "            dt_local = dt.astimezone(VN_TZ)\n",
    "        else:\n",
    "            dt_local = dt\n",
    "        return dt_local.date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def extract_language_from_html(soup: BeautifulSoup) -> str:\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            lang = lang.lower().strip()\n",
    "            if lang.startswith(\"vi\"):\n",
    "                return \"vi\"\n",
    "            if lang.startswith(\"en\"):\n",
    "                return \"en\"\n",
    "            return lang\n",
    "    return DEFAULT_LANGUAGE\n",
    "\n",
    "\n",
    "def extract_keywords_from_html(soup: BeautifulSoup) -> List[str]:\n",
    "    for sel in ['meta[name=\"keywords\"]', 'meta[name=\"news_keywords\"]']:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag and tag.get(\"content\"):\n",
    "            raw = tag[\"content\"]\n",
    "            kws = [x.strip() for x in raw.split(\",\") if x.strip()]\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for k in kws:\n",
    "                if k not in seen:\n",
    "                    seen.add(k)\n",
    "                    out.append(k)\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1.title-detail, h1.cate-24h-title-detail, h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = to_iso_utc(m_pub[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        m2 = soup.select_one('meta[itemprop=\"datePublished\"]')\n",
    "        if m2 and m2.get(\"content\"):\n",
    "            pub = to_iso_utc(m2[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        ttag = soup.select_one(\"time\")\n",
    "        if ttag:\n",
    "            pub = to_iso_utc(ttag.get(\"datetime\") or ttag.get_text(strip=True)) or \"\"\n",
    "    if not pub:\n",
    "        # 24h c√≥ th·ªÉ d√πng class .cate-24h-date-published\n",
    "        date_pub = soup.select_one(\".cate-24h-date-published\")\n",
    "        if date_pub:\n",
    "            pub = to_iso_utc(date_pub.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category.primary\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    language = extract_language_from_html(soup)\n",
    "    keywords = extract_keywords_from_html(soup)\n",
    "\n",
    "    # content.text - 24h th∆∞·ªùng d√πng class .cate-24h-content-text\n",
    "    content_text = \"\"\n",
    "    article_body = soup.select_one(\".cate-24h-content-text\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article .content-text\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".content-text\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".article-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    \n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    id_ = md5_id(url)\n",
    "    category_primary = meta.get(\"category_from_article\") or category_fallback\n",
    "    keywords_str = \"|\".join(meta.get(\"keywords\") or [])\n",
    "    entities_str = \"|\".join(meta.get(\"entities\") or [])\n",
    "\n",
    "    return {\n",
    "        \"id\": id_,\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": category_primary or \"\",\n",
    "        \"keywords\": keywords_str,\n",
    "        \"entities\": entities_str,\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_rss_feed(rss_url: str, category_slug: str, end_date: str, \n",
    "                   seen_urls: Set[str], seen_ids: Set[str]) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Crawl articles t·ª´ RSS feed\n",
    "    Returns: (added, skipped_duplicate, skipped_old)\n",
    "    \"\"\"\n",
    "    added = 0\n",
    "    skipped_old = 0\n",
    "    skipped_duplicate = 0\n",
    "    \n",
    "    try:\n",
    "        feed = fetch_rss(rss_url)\n",
    "    except Exception as e:\n",
    "        log(f\"[WARN] RSS fetch failed {rss_url}: {e}\")\n",
    "        return (0, 0, 0)\n",
    "    \n",
    "    if not feed.entries:\n",
    "        log(f\"[WARN] No entries in RSS feed {rss_url}\")\n",
    "        return (0, 0, 0)\n",
    "    \n",
    "    for entry in feed.entries:\n",
    "        article_url = entry.get(\"link\", \"\").strip()\n",
    "        if not article_url:\n",
    "            continue\n",
    "            \n",
    "        # Normalize URL\n",
    "        if not article_url.startswith(\"http\"):\n",
    "            article_url = \"https://www.24h.com.vn\" + article_url\n",
    "        \n",
    "        # L·∫•y published date t·ª´ RSS ƒë·ªÉ check tr∆∞·ªõc\n",
    "        pub_date_rss = entry.get(\"published\") or entry.get(\"updated\")\n",
    "        pub_iso_rss = to_iso_utc(pub_date_rss) if pub_date_rss else \"\"\n",
    "        pub_local_date = iso_to_local_date(pub_iso_rss) or \"\"\n",
    "        \n",
    "        # Skip articles older than END_DATE tr∆∞·ªõc khi check duplicate\n",
    "        # V√¨ RSS ƒë∆∞·ª£c s·∫Øp x·∫øp theo th·ªùi gian, c√≥ th·ªÉ early exit\n",
    "        if pub_local_date and pub_local_date < end_date:\n",
    "            skipped_old += 1\n",
    "            continue\n",
    "            \n",
    "        # Check duplicate - QUAN TR·ªåNG: Skip n·∫øu ƒë√£ crawl\n",
    "        # Khi ch·∫°y h√†ng ng√†y, ƒëa s·ªë articles s·∫Ω b·ªã skip ·ªü ƒë√¢y\n",
    "        if article_url in seen_urls:\n",
    "            skipped_duplicate += 1\n",
    "            continue\n",
    "            \n",
    "        aid = md5_id(article_url)\n",
    "        if aid in seen_ids:\n",
    "            skipped_duplicate += 1\n",
    "            continue\n",
    "        \n",
    "        # Fetch full article content (ch·ªâ v·ªõi articles m·ªõi)\n",
    "        try:\n",
    "            article_html = fetch_text(article_url)\n",
    "            meta = extract_article_meta(article_html)\n",
    "        except Exception as e:\n",
    "            log(f\"[WARN] article fetch failed {article_url}: {e}\")\n",
    "            # Fallback: use RSS data\n",
    "            meta = {\n",
    "                \"title\": entry.get(\"title\", \"\"),\n",
    "                \"published_at\": pub_iso_rss,\n",
    "                \"language\": DEFAULT_LANGUAGE,\n",
    "                \"keywords\": [],\n",
    "                \"category_from_article\": \"\",\n",
    "                \"entities\": [],\n",
    "                \"content_text\": BeautifulSoup(entry.get(\"summary\", \"\"), \"lxml\").get_text(strip=True),\n",
    "            }\n",
    "        finally:\n",
    "            polite_sleep()\n",
    "        \n",
    "        # Use RSS published date if article doesn't have one\n",
    "        if not meta.get(\"published_at\") and pub_iso_rss:\n",
    "            meta[\"published_at\"] = pub_iso_rss\n",
    "        \n",
    "        row = make_row(article_url, meta, category_fallback=category_slug)\n",
    "        append_row(CSV_PATH, row)\n",
    "        seen_urls.add(article_url)\n",
    "        seen_ids.add(aid)\n",
    "        added += 1\n",
    "    \n",
    "    # Always show summary for transparency\n",
    "    print(f\"  [{category_slug}] RSS entries: {len(feed.entries)} | Added: {added} | Duplicates: {skipped_duplicate} | Old: {skipped_old}\")\n",
    "    \n",
    "    return (added, skipped_duplicate, skipped_old)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"24H.COM.VN CRAWLER - Duplicate-Safe Daily Crawling\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "    \n",
    "    print(f\"\\nüìä Initial state:\")\n",
    "    print(f\"  - Already crawled: {len(seen_urls)} URLs, {len(seen_ids)} IDs\")\n",
    "    print(f\"  - Date filter: Articles >= {END_DATE}\")\n",
    "    print(f\"  - Total feeds: {len(RSS_FEEDS)}\")\n",
    "    print()\n",
    "\n",
    "    total_added = 0\n",
    "    total_duplicates = 0\n",
    "    total_old = 0\n",
    "    \n",
    "    for rss_url, category_slug in RSS_FEEDS:\n",
    "        try:\n",
    "            added, duplicates, old = crawl_rss_feed(rss_url, category_slug, END_DATE, seen_urls, seen_ids)\n",
    "            total_added += added\n",
    "            total_duplicates += duplicates\n",
    "            total_old += old\n",
    "        except Exception as e:\n",
    "            print(f\"  [{category_slug}] ERROR: {e}\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ CRAWL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìù New articles added: {total_added}\")\n",
    "    print(f\"üîÅ Duplicates skipped: {total_duplicates} (already in CSV)\")\n",
    "    print(f\"‚è∞ Old articles skipped: {total_old} (before {END_DATE})\")\n",
    "    print(f\"üìä Total processed: {total_added + total_duplicates + total_old}\")\n",
    "    print(f\"üíæ Output: {CSV_PATH}\")\n",
    "    print(f\"üìà Total in CSV now: {len(seen_urls) + total_added} articles\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if total_duplicates > 0:\n",
    "        efficiency = (total_duplicates / (total_added + total_duplicates + total_old) * 100) if (total_added + total_duplicates + total_old) > 0 else 0\n",
    "        print(f\"\\nüí° Duplicate rate: {efficiency:.1f}% - Perfect for daily runs!\")\n",
    "        print(f\"   (High rate = most articles already crawled = efficient)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86e88e",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch v·ªÅ x·ª≠ l√Ω Duplicate khi ch·∫°y h√†ng ng√†y\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**: Khi ch·∫°y crawler h√†ng ng√†y, RSS feed s·∫Ω c√≥ nhi·ªÅu articles ƒë√£ crawl ng√†y h√¥m tr∆∞·ªõc\n",
    "\n",
    "**V√≠ d·ª•**:\n",
    "- Ng√†y 1 (4/2): RSS c√≥ 109 articles t·ª´ 30/1 ‚Üí 4/2, crawl ƒë∆∞·ª£c 228 articles\n",
    "- Ng√†y 2 (5/2): RSS c√≥ 109 articles t·ª´ 31/1 ‚Üí 5/2, trong ƒë√≥ ~90 articles ƒë√£ c√≥\n",
    "- ‚Üí C·∫ßn skip ~90 articles duplicate, ch·ªâ crawl ~19 articles m·ªõi\n",
    "\n",
    "**C√°ch x·ª≠ l√Ω trong code**:\n",
    "\n",
    "1. **Load seen data**: `load_seen_from_csv()` ƒë·ªçc t·∫•t c·∫£ URLs ƒë√£ crawl t·ª´ CSV\n",
    "2. **Check duplicate TR∆Ø·ªöC fetch**: \n",
    "   - Check `if article_url in seen_urls` ‚Üí skip ngay\n",
    "   - Kh√¥ng fetch HTML c·ªßa articles ƒë√£ c√≥ ‚Üí ti·∫øt ki·ªám bandwidth\n",
    "3. **Ch·ªâ crawl articles m·ªõi**: Fetch v√† parse HTML ch·ªâ v·ªõi articles ch∆∞a c√≥\n",
    "\n",
    "**K·∫øt qu·∫£**: \n",
    "- L·∫ßn ch·∫°y ƒë·∫ßu: Crawl 200+ articles (~3 ph√∫t)\n",
    "- L·∫ßn ch·∫°y sau: Ch·ªâ crawl 10-20 articles m·ªõi (~30 gi√¢y)\n",
    "- An to√†n: Kh√¥ng bao gi·ªù duplicate data trong CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
