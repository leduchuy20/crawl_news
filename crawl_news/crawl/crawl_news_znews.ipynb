{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b058a855",
   "metadata": {},
   "source": [
    "# ZNews HTML Category Crawler - V·ªõi C∆° ch·∫ø x·ª≠ l√Ω Duplicate\n",
    "\n",
    "Crawler n√†y s·∫Ω thu th·∫≠p tin t·ª©c t·ª´ ZNews theo t·ª´ng category b·∫±ng c√°ch:\n",
    "1. **T·∫£i danh s√°ch ƒë√£ crawl**: Load t·∫•t c·∫£ URL v√† ID ƒë√£ c√≥ trong CSV v√†o b·ªô nh·ªõ\n",
    "2. **Ki·ªÉm tra tr∆∞·ªõc khi fetch**: Tr∆∞·ªõc khi t·∫£i HTML c·ªßa b√†i vi·∫øt, ki·ªÉm tra xem URL ƒë√£ t·ªìn t·∫°i ch∆∞a\n",
    "3. **B·ªè qua duplicate t·ª± ƒë·ªông**: N·∫øu URL ho·∫∑c ID ƒë√£ c√≥ trong CSV, b·ªè qua ngay l·∫≠p t·ª©c (kh√¥ng t·∫£i HTML)\n",
    "4. **Ch·ªâ crawl b√†i m·ªõi**: Ch·ªâ t·∫£i v√† x·ª≠ l√Ω HTML c·ªßa nh·ªØng b√†i vi·∫øt ch∆∞a c√≥ trong CSV\n",
    "5. **B√°o c√°o th·ªëng k√™**: Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng b√†i m·ªõi, b√†i b·ªã tr√πng, b√†i c≈©, v√† t·ª∑ l·ªá duplicate\n",
    "\n",
    "## V√≠ d·ª• Output khi ch·∫°y h√†ng ng√†y:\n",
    "\n",
    "**L·∫ßn 1 (ng√†y ƒë·∫ßu):**\n",
    "```\n",
    "[https://znews.vn/xuat-ban.html] added=45, duplicates=0, old=5 (0.0% duplicate)\n",
    "[https://znews.vn/kinh-doanh-tai-chinh.html] added=52, duplicates=0, old=3 (0.0% duplicate)\n",
    "Total: 320 new, 0 duplicates, 25 old (0.0% duplicate rate)\n",
    "```\n",
    "\n",
    "**L·∫ßn 2 (ng√†y th·ª© 2):**\n",
    "```\n",
    "[https://znews.vn/xuat-ban.html] added=8, duplicates=40, old=7 (83.3% duplicate)\n",
    "[https://znews.vn/kinh-doanh-tai-chinh.html] added=12, duplicates=45, old=5 (78.9% duplicate)\n",
    "Total: 65 new, 280 duplicates, 30 old (81.2% duplicate rate)\n",
    "```\n",
    "\n",
    "**L·∫ßn 3 (ng√†y th·ª© 3):**\n",
    "```\n",
    "[https://znews.vn/xuat-ban.html] added=5, duplicates=42, old=8 (89.4% duplicate)\n",
    "[https://znews.vn/kinh-doanh-tai-chinh.html] added=7, duplicates=50, old=6 (87.7% duplicate)\n",
    "Total: 45 new, 295 duplicates, 35 old (86.8% duplicate rate)\n",
    "```\n",
    "\n",
    "## L·ª£i √≠ch:\n",
    "- ‚úÖ **Kh√¥ng c√≥ duplicate trong CSV** - ƒê·∫£m b·∫£o m·ªói URL ch·ªâ xu·∫•t hi·ªán 1 l·∫ßn\n",
    "- ‚úÖ **Ch·∫°y nhanh** - B·ªè qua duplicate ngay tr∆∞·ªõc khi fetch HTML (ti·∫øt ki·ªám th·ªùi gian v√† bƒÉng th√¥ng)\n",
    "- ‚úÖ **T·ª± ƒë·ªông** - Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng, ch·ªâ c·∫ßn ch·∫°y script h√†ng ng√†y\n",
    "- ‚úÖ **B√°o c√°o r√µ r√†ng** - Bi·∫øt ch√≠nh x√°c bao nhi√™u b√†i m·ªõi, bao nhi√™u b√†i tr√πng\n",
    "- ‚úÖ **Hi·ªáu qu·∫£ cao** - T·ª∑ l·ªá duplicate tƒÉng d·∫ßn sau m·ªói l·∫ßn ch·∫°y, ch·ª©ng t·ªè crawler ƒëang ho·∫°t ƒë·ªông ƒë√∫ng\n",
    "\n",
    "## Khuy·∫øn ngh·ªã:\n",
    "- Ch·∫°y crawler **m·ªói ng√†y 1 l·∫ßn** ƒë·ªÉ c·∫≠p nh·∫≠t tin t·ª©c m·ªõi nh·∫•t\n",
    "- ZNews c·∫≠p nh·∫≠t tin trong v√≤ng 24h, n√™n duplicate rate s·∫Ω cao (80-90%) sau 2-3 ng√†y ƒë·∫ßu\n",
    "- T·ª∑ l·ªá duplicate cao = crawler ƒëang ho·∫°t ƒë·ªông hi·ªáu qu·∫£ (kh√¥ng b·ªè s√≥t b√†i c≈©, kh√¥ng crawl l·∫°i b√†i ƒë√£ c√≥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f58eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZNews Crawler - C∆° ch·∫ø x·ª≠ l√Ω Duplicate ===\n",
      "ƒê√£ load 2851 URLs v√† 2851 IDs t·ª´ CSV\n",
      "END_DATE: 2026-01-15\n",
      "Crawling 7 categories...\n",
      "\n",
      "[https://znews.vn/xuat-ban.html]\n",
      "  ‚úÖ Added: 13 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 87 b√†i tr√πng\n",
      "  ‚è∞ Old: 12 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 77.7%\n",
      "\n",
      "[https://znews.vn/kinh-doanh-tai-chinh.html]\n",
      "  ‚úÖ Added: 35 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 75 b√†i tr√πng\n",
      "  ‚è∞ Old: 12 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 61.5%\n",
      "\n",
      "[https://znews.vn/suc-khoe.html]\n",
      "  ‚úÖ Added: 16 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 98 b√†i tr√πng\n",
      "  ‚è∞ Old: 28 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 69.0%\n",
      "\n",
      "[https://znews.vn/the-thao.html]\n",
      "  ‚úÖ Added: 49 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 54 b√†i tr√πng\n",
      "  ‚è∞ Old: 6 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 49.5%\n",
      "\n",
      "[https://znews.vn/doi-song.html]\n",
      "  ‚úÖ Added: 20 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 81 b√†i tr√πng\n",
      "  ‚è∞ Old: 30 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 61.8%\n",
      "\n",
      "[https://znews.vn/cong-nghe.html]\n",
      "  ‚úÖ Added: 7 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 93 b√†i tr√πng\n",
      "  ‚è∞ Old: 6 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 87.7%\n",
      "\n",
      "[https://znews.vn/giai-tri.html]\n",
      "  ‚úÖ Added: 22 b√†i m·ªõi\n",
      "  üîÑ Duplicates: 80 b√†i tr√πng\n",
      "  ‚è∞ Old: 34 b√†i c≈© (< 2026-01-15)\n",
      "  üìä Duplicate rate: 58.8%\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìà T·ªîNG K·∫æT:\n",
      "  ‚úÖ T·ªïng b√†i m·ªõi th√™m v√†o CSV: 162\n",
      "  üîÑ T·ªïng b√†i tr√πng (b·ªè qua): 568\n",
      "  ‚è∞ T·ªïng b√†i c≈© (b·ªè qua): 128\n",
      "  üìä T·ªïng b√†i ki·ªÉm tra: 858\n",
      "  üíØ T·ª∑ l·ªá duplicate: 66.2%\n",
      "============================================================\n",
      "\n",
      "‚úÖ Ho√†n th√†nh! ƒê√£ th√™m 162 b√†i m·ªõi v√†o znews_html_categories_vi.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from turtle import pd\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    \"https://znews.vn/xuat-ban.html\",\n",
    "    \"https://znews.vn/kinh-doanh-tai-chinh.html\",\n",
    "    \"https://znews.vn/suc-khoe.html\",\n",
    "    \"https://znews.vn/the-thao.html\",\n",
    "    \"https://znews.vn/doi-song.html\",\n",
    "    \"https://znews.vn/cong-nghe.html\",\n",
    "    \"https://znews.vn/giai-tri.html\",\n",
    "]\n",
    "\n",
    "# Crawl t·ª´ m·ªõi -> c≈© cho t·ªõi khi b√†i c√≥ ng√†y < END_DATE (theo gi·ªù VN)\n",
    "END_DATE = \"2026-01-15\"  # YYYY-MM-DD\n",
    "MAX_PAGES_PER_CATEGORY = 2000  # safety stop\n",
    "\n",
    "CSV_PATH = \"znews_html_categories_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.25\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; ZNewsHTMLCrawler/1.0)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"ZNews\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    r = session.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def to_iso_utc(s: Optional[str]) -> Optional[str]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(s)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            if VN_TZ:\n",
    "                dt = dt.replace(tzinfo=VN_TZ)\n",
    "            else:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(iso_utc)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        if VN_TZ:\n",
    "            dt_local = dt.astimezone(VN_TZ)\n",
    "        else:\n",
    "            dt_local = dt\n",
    "        return dt_local.date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def extract_language_from_html(soup: BeautifulSoup) -> str:\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\") or html_tag.get(\"xml:lang\")\n",
    "        if lang:\n",
    "            lang = lang.lower().strip()\n",
    "            if lang.startswith(\"vi\"):\n",
    "                return \"vi\"\n",
    "            if lang.startswith(\"en\"):\n",
    "                return \"en\"\n",
    "            return lang\n",
    "    return DEFAULT_LANGUAGE\n",
    "\n",
    "\n",
    "def extract_keywords_from_html(soup: BeautifulSoup) -> List[str]:\n",
    "    for sel in ['meta[name=\"keywords\"]', 'meta[name=\"news_keywords\"]']:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag and tag.get(\"content\"):\n",
    "            raw = tag[\"content\"]\n",
    "            kws = [x.strip() for x in raw.split(\",\") if x.strip()]\n",
    "            seen = set()\n",
    "            out = []\n",
    "            for k in kws:\n",
    "                if k not in seen:\n",
    "                    seen.add(k)\n",
    "                    out.append(k)\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1.title-detail, h1.article-title, h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = to_iso_utc(m_pub[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        m2 = soup.select_one('meta[itemprop=\"datePublished\"]')\n",
    "        if m2 and m2.get(\"content\"):\n",
    "            pub = to_iso_utc(m2[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        ttag = soup.select_one(\"time\")\n",
    "        if ttag:\n",
    "            pub = to_iso_utc(ttag.get(\"datetime\") or ttag.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category.primary\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    language = extract_language_from_html(soup)\n",
    "    keywords = extract_keywords_from_html(soup)\n",
    "\n",
    "    # content.text - ZNews th∆∞·ªùng d√πng class .article-body, .the-article-body\n",
    "    content_text = \"\"\n",
    "    article_body = soup.select_one(\".the-article-body\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".article-body\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".content-detail\")\n",
    "    \n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_article_urls_from_category_page(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    urls = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://znews.vn\" + href\n",
    "        if not href.startswith(\"https://znews.vn/\"):\n",
    "            continue\n",
    "        # ZNews b√†i vi·∫øt th∆∞·ªùng c√≥ format /ten-bai-post[ID].html\n",
    "        if \"-post\" in href and \".html\" in href:\n",
    "            urls.append(href.split(\"?\")[0])\n",
    "\n",
    "    # unique gi·ªØ th·ª© t·ª±\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "\n",
    "def find_next_page_url(category_url: str, html: str, current_page: int) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # th·ª≠ rel=next\n",
    "    ln = soup.select_one('link[rel=\"next\"]')\n",
    "    if ln and ln.get(\"href\"):\n",
    "        href = ln[\"href\"].strip()\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://znews.vn\" + href\n",
    "        return href\n",
    "\n",
    "    # th·ª≠ t√¨m n√∫t pagination\n",
    "    a_next = soup.select_one('a.page-next, a[rel=\"next\"]')\n",
    "    if a_next and a_next.get(\"href\"):\n",
    "        href = a_next[\"href\"].strip()\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://znews.vn\" + href\n",
    "        return href\n",
    "\n",
    "    # fallback: ZNews d√πng format /trangX.html\n",
    "    # https://znews.vn/the-thao.html -> https://znews.vn/the-thao/trang2.html\n",
    "    next_page = current_page + 1\n",
    "    base_url = category_url.rstrip(\"/\").replace(\".html\", \"\")\n",
    "    return f\"{base_url}/trang{next_page}.html\"\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    id_ = md5_id(url)\n",
    "    category_primary = meta.get(\"category_from_article\") or category_fallback\n",
    "    keywords_str = \"|\".join(meta.get(\"keywords\") or [])\n",
    "    entities_str = \"|\".join(meta.get(\"entities\") or [])\n",
    "\n",
    "    return {\n",
    "        \"id\": id_,\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": category_primary or \"\",\n",
    "        \"keywords\": keywords_str,\n",
    "        \"entities\": entities_str,\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Crawl category v√† tr·∫£ v·ªÅ (added, skipped_duplicate, skipped_old)\n",
    "    - added: s·ªë b√†i m·ªõi ƒë∆∞·ª£c th√™m v√†o CSV\n",
    "    - skipped_duplicate: s·ªë b√†i b·ªã tr√πng (ƒë√£ c√≥ trong CSV)\n",
    "    - skipped_old: s·ªë b√†i c≈© h∆°n END_DATE\n",
    "    \"\"\"\n",
    "    added = 0\n",
    "    skipped_duplicate = 0\n",
    "    skipped_old = 0\n",
    "    page = 1\n",
    "    url_page = category_url\n",
    "\n",
    "    # extract category slug t·ª´ URL\n",
    "    category_slug = category_url.rstrip(\"/\").split(\"/\")[-1].replace(\".html\", \"\")\n",
    "\n",
    "    while page <= MAX_PAGES_PER_CATEGORY and url_page:\n",
    "        html = fetch_text(url_page)\n",
    "        article_urls = extract_article_urls_from_category_page(html)\n",
    "\n",
    "        if DEBUG:\n",
    "            log(f\"[{category_slug}] page {page} got {len(article_urls)} candidate urls: {url_page}\")\n",
    "\n",
    "        if not article_urls:\n",
    "            break\n",
    "\n",
    "        page_all_older_than_end = True\n",
    "\n",
    "        for aurl in article_urls:\n",
    "            # Ki·ªÉm tra duplicate TR∆Ø·ªöC KHI fetch HTML\n",
    "            if aurl in seen_urls:\n",
    "                skipped_duplicate += 1\n",
    "                continue\n",
    "\n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                skipped_duplicate += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "\n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "\n",
    "            # n·∫øu c√≥ ng√†y v√† nh·ªè h∆°n end_date => ƒë√°nh d·∫•u c≈©\n",
    "            if pub_local_date and pub_local_date < end_date:\n",
    "                skipped_old += 1\n",
    "                pass\n",
    "            else:\n",
    "                page_all_older_than_end = False\n",
    "\n",
    "            # N·∫øu b√†i >= end_date th√¨ ghi\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "\n",
    "        # N·∫øu c·∫£ trang to√†n b√†i c≈© h∆°n end_date th√¨ d·ª´ng category n√†y\n",
    "        if page_all_older_than_end:\n",
    "            if DEBUG:\n",
    "                log(f\"[{category_slug}] stop: page {page} all older than end_date={end_date}\")\n",
    "            break\n",
    "\n",
    "        # ƒëi trang ti·∫øp\n",
    "        next_url = find_next_page_url(category_url, html, current_page=page)\n",
    "        if not next_url or next_url == url_page:\n",
    "            break\n",
    "        if next_url == url_page:\n",
    "            break\n",
    "        url_page = next_url\n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "\n",
    "    return added, skipped_duplicate, skipped_old\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "\n",
    "    print(f\"=== ZNews Crawler - C∆° ch·∫ø x·ª≠ l√Ω Duplicate ===\")\n",
    "    print(f\"ƒê√£ load {len(seen_urls)} URLs v√† {len(seen_ids)} IDs t·ª´ CSV\")\n",
    "    print(f\"END_DATE: {END_DATE}\")\n",
    "    print(f\"Crawling {len(CATEGORY_URLS)} categories...\\n\")\n",
    "\n",
    "    total_added = 0\n",
    "    total_duplicates = 0\n",
    "    total_old = 0\n",
    "    \n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added, skipped_duplicate, skipped_old = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            total_added += added\n",
    "            total_duplicates += skipped_duplicate\n",
    "            total_old += skipped_old\n",
    "            \n",
    "            # T√≠nh t·ª∑ l·ªá duplicate\n",
    "            total_found = added + skipped_duplicate + skipped_old\n",
    "            dup_rate = (skipped_duplicate / total_found * 100) if total_found > 0 else 0.0\n",
    "            \n",
    "            print(f\"[{cat}]\")\n",
    "            print(f\"  ‚úÖ Added: {added} b√†i m·ªõi\")\n",
    "            print(f\"  üîÑ Duplicates: {skipped_duplicate} b√†i tr√πng\")\n",
    "            print(f\"  ‚è∞ Old: {skipped_old} b√†i c≈© (< {END_DATE})\")\n",
    "            print(f\"  üìä Duplicate rate: {dup_rate:.1f}%\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{cat}] ‚ùå ERROR: {e}\\n\")\n",
    "\n",
    "    # T·ªïng k·∫øt\n",
    "    grand_total = total_added + total_duplicates + total_old\n",
    "    overall_dup_rate = (total_duplicates / grand_total * 100) if grand_total > 0 else 0.0\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(\"znews_html_categories_vi.csv\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìà T·ªîNG K·∫æT:\")\n",
    "    print(f\"  ‚úÖ T·ªïng b√†i: {len(df)}\")\n",
    "    print(f\"  ‚úÖ T·ªïng b√†i m·ªõi th√™m v√†o CSV: {total_added}\")\n",
    "    print(f\"  üîÑ T·ªïng b√†i tr√πng (b·ªè qua): {total_duplicates}\")\n",
    "    print(f\"  ‚è∞ T·ªïng b√†i c≈© (b·ªè qua): {total_old}\")\n",
    "    print(f\"  üìä T·ªïng b√†i ki·ªÉm tra: {grand_total}\")\n",
    "    print(f\"  üíØ T·ª∑ l·ªá duplicate: {overall_dup_rate:.1f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n‚úÖ Ho√†n th√†nh! ƒê√£ th√™m {total_added} b√†i m·ªõi v√†o {CSV_PATH}\")\n",
    "    \n",
    "    if overall_dup_rate > 70:\n",
    "        print(f\"üí° G·ª£i √Ω: T·ª∑ l·ªá duplicate cao ({overall_dup_rate:.1f}%) cho th·∫•y crawler ƒëang ho·∫°t ƒë·ªông t·ªët!\")\n",
    "    elif overall_dup_rate < 20 and total_duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  L∆∞u √Ω: T·ª∑ l·ªá duplicate th·∫•p ({overall_dup_rate:.1f}%) - c√≥ th·ªÉ c√≥ nhi·ªÅu b√†i m·ªõi ho·∫∑c ngu·ªìn c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"znews_html_categories_vi.csv\")\n",
    "print(f\"  ‚úÖ T·ªïng b√†i: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
