{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4960447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\work\\big_data\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\work\\big_data\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: lxml in d:\\work\\big_data\\.venv\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: python-dateutil in d:\\work\\big_data\\.venv\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: feedparser in d:\\work\\big_data\\.venv\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\big_data\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\work\\big_data\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\big_data\\.venv\\lib\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in d:\\work\\big_data\\.venv\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml python-dateutil feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385087e",
   "metadata": {},
   "source": [
    "## Code Crawler ƒê·∫ßy ƒê·ªß v·ªõi Cookie Handling\n",
    "\n",
    "Code d∆∞·ªõi ƒë√¢y ƒë√£ ƒë∆∞·ª£c fix ƒë·ªÉ x·ª≠ l√Ω cookie protection c·ªßa laodong.vn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b450cdf",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è C∆° ch·∫ø x·ª≠ l√Ω Duplicate khi ch·∫°y h√†ng ng√†y\n",
    "\n",
    "Code crawler b√™n d∆∞·ªõi **ƒê√É C√ì S·∫¥N** c∆° ch·∫ø x·ª≠ l√Ω duplicate ho√†n ch·ªânh:\n",
    "\n",
    "### ‚úÖ C√°c b∆∞·ªõc x·ª≠ l√Ω:\n",
    "\n",
    "1. **Load d·ªØ li·ªáu c≈© t·ª´ CSV** (h√†m `load_seen_from_csv`):\n",
    "   - ƒê·ªçc t·∫•t c·∫£ URLs ƒë√£ crawl ‚Üí `seen_urls` set\n",
    "   - ƒê·ªçc t·∫•t c·∫£ IDs ƒë√£ crawl ‚Üí `seen_ids` set\n",
    "   - L∆∞u trong memory ƒë·ªÉ check nhanh\n",
    "\n",
    "2. **Check duplicate TR∆Ø·ªöC khi fetch** (h√†m `crawl_rss_feed`):\n",
    "   ```python\n",
    "   if url in seen_urls:\n",
    "       skipped_duplicate += 1\n",
    "       continue  # Skip ngay, kh√¥ng fetch HTML\n",
    "   \n",
    "   aid = md5_id(url)\n",
    "   if aid in seen_ids:\n",
    "       skipped_duplicate += 1\n",
    "       continue  # Skip ngay\n",
    "   ```\n",
    "\n",
    "3. **Ch·ªâ fetch & parse HTML v·ªõi articles M·ªöI**:\n",
    "   - Ti·∫øt ki·ªám th·ªùi gian (kh√¥ng fetch b√†i c≈©)\n",
    "   - Ti·∫øt ki·ªám bandwidth\n",
    "   - Ch·∫°y nhanh h∆°n nhi·ªÅu l·∫ßn\n",
    "\n",
    "4. **B√°o c√°o chi ti·∫øt**:\n",
    "   - `Added`: S·ªë b√†i m·ªõi th√™m v√†o\n",
    "   - `Duplicates`: S·ªë b√†i ƒë√£ c√≥ (skip)\n",
    "   - `Old`: S·ªë b√†i c≈© h∆°n END_DATE (skip)\n",
    "\n",
    "### üìä K·∫øt qu·∫£ khi ch·∫°y h√†ng ng√†y:\n",
    "\n",
    "**L·∫ßn 1** (4/2/2026):\n",
    "```\n",
    "[home] RSS entries: 40 | Added: 38 | Duplicates: 0 | Old: 2\n",
    "Total: 350 articles added\n",
    "```\n",
    "\n",
    "**L·∫ßn 2** (5/2/2026):\n",
    "```\n",
    "[home] RSS entries: 40 | Added: 5 | Duplicates: 33 | Old: 2\n",
    "Total: 50 articles added (ch·∫°y nhanh ~30 gi√¢y)\n",
    "\n",
    "üí° Duplicate rate: 85.2% - Perfect for daily runs!\n",
    "```\n",
    "\n",
    "**L·∫ßn 3** (6/2/2026):\n",
    "```\n",
    "[home] RSS entries: 40 | Added: 6 | Duplicates: 32 | Old: 2\n",
    "Total: 60 articles added\n",
    "```\n",
    "\n",
    "### üéØ L·ª£i √≠ch:\n",
    "\n",
    "‚úÖ **Kh√¥ng bao gi·ªù duplicate** trong CSV  \n",
    "‚úÖ **Ch·∫°y c·ª±c nhanh** khi c√≥ nhi·ªÅu b√†i tr√πng  \n",
    "‚úÖ **T·ª± ƒë·ªông skip** b√†i ƒë√£ c√≥  \n",
    "‚úÖ **B√°o c√°o r√µ r√†ng** ƒë·ªÉ theo d√µi\n",
    "\n",
    "### üí° Khuy·∫øn ngh·ªã:\n",
    "\n",
    "- Ch·∫°y crawler **m·ªói ng√†y 1 l·∫ßn** (v√†o s√°ng s·ªõm)\n",
    "- RSS feeds th∆∞·ªùng c·∫≠p nh·∫≠t b√†i m·ªõi trong 24h\n",
    "- Duplicate rate cao (>80%) = hi·ªáu qu·∫£ t·ªët!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45a3e4",
   "metadata": {},
   "source": [
    "## Gi·∫£i th√≠ch v·∫•n ƒë·ªÅ \"ch·ªâ crawl ƒë∆∞·ª£c 1 ng√†y\"\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:** RSS feeds ch·ªâ ch·ª©a ~40 b√†i **m·ªõi nh·∫•t** (th∆∞·ªùng l√† trong 1 ng√†y). RSS kh√¥ng ph·∫£i c√¥ng c·ª• ƒë·ªÉ crawl d·ªØ li·ªáu l·ªãch s·ª≠!\n",
    "\n",
    "**Gi·∫£i ph√°p:**\n",
    "1. **Ch·∫°y scheduler h√†ng ng√†y** ƒë·ªÉ t√≠ch l≈©y d·ªØ li·ªáu li√™n t·ª•c\n",
    "2. **Ho·∫∑c d√πng HTML crawler** ƒë·ªÉ crawl theo category pages v·ªõi pagination (nh∆∞ vnexpress/dantri)\n",
    "\n",
    "B√™n d∆∞·ªõi l√† **HTML crawler** ƒë·ªÉ l·∫•y nhi·ªÅu ng√†y d·ªØ li·ªáu h∆°n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed7b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LaoDong HTML crawler...\n",
      "END_DATE: 2026-01-20\n",
      "CSV: laodong_html_articles_vi.csv\n",
      "Categories: 13\n",
      "\n",
      "[thoi-su] Starting crawl...\n",
      "[thoi-su] Fetching page 1: https://laodong.vn/thoi-su/\n",
      "[thoi-su] Page 1: Found 29 candidate articles\n",
      "[thoi-su] Page 1: Added 22 articles (total: 22)\n",
      "[thoi-su] Fetching page 2: https://laodong.vn/thoi-su?page=2\n",
      "[thoi-su] Page 2: Found 29 candidate articles\n",
      "[thoi-su] Page 2: Added 3 articles (total: 25)\n",
      "[thoi-su] Fetching page 3: https://laodong.vn/thoi-su?page=3\n",
      "[thoi-su] Page 3: Found 30 candidate articles\n",
      "[thoi-su] Page 3: Added 0 articles (total: 25)\n",
      "[thoi-su] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/thoi-su/] Added 25 articles\n",
      "\n",
      "\n",
      "[the-gioi] Starting crawl...\n",
      "[the-gioi] Fetching page 1: https://laodong.vn/the-gioi/\n",
      "[the-gioi] Page 1: Found 30 candidate articles\n",
      "[the-gioi] Page 1: Added 20 articles (total: 20)\n",
      "[the-gioi] Fetching page 2: https://laodong.vn/the-gioi?page=2\n",
      "[the-gioi] Page 2: Found 30 candidate articles\n",
      "[the-gioi] Page 2: Added 1 articles (total: 21)\n",
      "[the-gioi] Fetching page 3: https://laodong.vn/the-gioi?page=3\n",
      "[the-gioi] Page 3: Found 30 candidate articles\n",
      "[the-gioi] Page 3: Added 0 articles (total: 21)\n",
      "[the-gioi] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/the-gioi/] Added 21 articles\n",
      "\n",
      "\n",
      "[xa-hoi] Starting crawl...\n",
      "[xa-hoi] Fetching page 1: https://laodong.vn/xa-hoi/\n",
      "[xa-hoi] Page 1: Found 31 candidate articles\n",
      "[xa-hoi] Page 1: Added 25 articles (total: 25)\n",
      "[xa-hoi] Fetching page 2: https://laodong.vn/xa-hoi?page=2\n",
      "[xa-hoi] Page 2: Found 31 candidate articles\n",
      "[xa-hoi] Page 2: Added 13 articles (total: 38)\n",
      "[xa-hoi] Fetching page 3: https://laodong.vn/xa-hoi?page=3\n",
      "[xa-hoi] Page 3: Found 31 candidate articles\n",
      "[xa-hoi] Page 3: Added 13 articles (total: 51)\n",
      "[xa-hoi] Fetching page 4: https://laodong.vn/xa-hoi?page=4\n",
      "[xa-hoi] Page 4: Found 33 candidate articles\n",
      "[xa-hoi] Page 4: Added 15 articles (total: 66)\n",
      "[xa-hoi] Fetching page 5: https://laodong.vn/xa-hoi?page=5\n",
      "[xa-hoi] Page 5: Found 33 candidate articles\n",
      "[xa-hoi] Page 5: Added 0 articles (total: 66)\n",
      "[xa-hoi] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/xa-hoi/] Added 66 articles\n",
      "\n",
      "\n",
      "[phap-luat] Starting crawl...\n",
      "[phap-luat] Fetching page 1: https://laodong.vn/phap-luat/\n",
      "[phap-luat] Page 1: Found 33 candidate articles\n",
      "[phap-luat] Page 1: Added 21 articles (total: 21)\n",
      "[phap-luat] Fetching page 2: https://laodong.vn/phap-luat?page=2\n",
      "[phap-luat] Page 2: Found 33 candidate articles\n",
      "[phap-luat] Page 2: Added 11 articles (total: 32)\n",
      "[phap-luat] Fetching page 3: https://laodong.vn/phap-luat?page=3\n",
      "[phap-luat] Page 3: Found 34 candidate articles\n",
      "[phap-luat] Page 3: Added 0 articles (total: 32)\n",
      "[phap-luat] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/phap-luat/] Added 32 articles\n",
      "\n",
      "\n",
      "[kinh-doanh] Starting crawl...\n",
      "[kinh-doanh] Fetching page 1: https://laodong.vn/kinh-doanh/\n",
      "[kinh-doanh] Page 1: Found 34 candidate articles\n",
      "[kinh-doanh] Page 1: Added 24 articles (total: 24)\n",
      "[kinh-doanh] Fetching page 2: https://laodong.vn/kinh-doanh?page=2\n",
      "[kinh-doanh] Page 2: Found 32 candidate articles\n",
      "[kinh-doanh] Page 2: Added 11 articles (total: 35)\n",
      "[kinh-doanh] Fetching page 3: https://laodong.vn/kinh-doanh?page=3\n",
      "[kinh-doanh] Page 3: Found 36 candidate articles\n",
      "[kinh-doanh] Page 3: Added 8 articles (total: 43)\n",
      "[kinh-doanh] Fetching page 4: https://laodong.vn/kinh-doanh?page=4\n",
      "[kinh-doanh] Page 4: Found 36 candidate articles\n",
      "[kinh-doanh] Page 4: Added 0 articles (total: 43)\n",
      "[kinh-doanh] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/kinh-doanh/] Added 43 articles\n",
      "\n",
      "\n",
      "[bat-dong-san] Starting crawl...\n",
      "[bat-dong-san] Fetching page 1: https://laodong.vn/bat-dong-san/\n",
      "[bat-dong-san] Page 1: Found 34 candidate articles\n",
      "[bat-dong-san] Page 1: Added 19 articles (total: 19)\n",
      "[bat-dong-san] Fetching page 2: https://laodong.vn/bat-dong-san?page=2\n",
      "[bat-dong-san] Page 2: Found 34 candidate articles\n",
      "[bat-dong-san] Page 2: Added 0 articles (total: 19)\n",
      "[bat-dong-san] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/bat-dong-san/] Added 19 articles\n",
      "\n",
      "\n",
      "[van-hoa] Starting crawl...\n",
      "[van-hoa] Fetching page 1: https://laodong.vn/van-hoa/\n",
      "[van-hoa] Page 1: Found 38 candidate articles\n",
      "[van-hoa] Page 1: Added 9 articles (total: 9)\n",
      "[van-hoa] Fetching page 2: https://laodong.vn/van-hoa?page=2\n",
      "[van-hoa] Page 2: Found 38 candidate articles\n",
      "[van-hoa] Page 2: Added 0 articles (total: 9)\n",
      "[van-hoa] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/van-hoa/] Added 9 articles\n",
      "\n",
      "\n",
      "[giao-duc] Starting crawl...\n",
      "[giao-duc] Fetching page 1: https://laodong.vn/giao-duc/\n",
      "[giao-duc] Page 1: Found 34 candidate articles\n",
      "[giao-duc] Page 1: Added 20 articles (total: 20)\n",
      "[giao-duc] Fetching page 2: https://laodong.vn/giao-duc?page=2\n",
      "[giao-duc] Page 2: Found 32 candidate articles\n",
      "[giao-duc] Page 2: Added 5 articles (total: 25)\n",
      "[giao-duc] Fetching page 3: https://laodong.vn/giao-duc?page=3\n",
      "[giao-duc] Page 3: Found 33 candidate articles\n",
      "[giao-duc] Page 3: Added 4 articles (total: 29)\n",
      "[giao-duc] Fetching page 4: https://laodong.vn/giao-duc?page=4\n",
      "[giao-duc] Page 4: Found 34 candidate articles\n",
      "[giao-duc] Page 4: Added 6 articles (total: 35)\n",
      "[giao-duc] Fetching page 5: https://laodong.vn/giao-duc?page=5\n",
      "[giao-duc] Page 5: Found 34 candidate articles\n",
      "[giao-duc] Page 5: Added 0 articles (total: 35)\n",
      "[giao-duc] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/giao-duc/] Added 35 articles\n",
      "\n",
      "\n",
      "[the-thao] Starting crawl...\n",
      "[the-thao] Fetching page 1: https://laodong.vn/the-thao/\n",
      "[the-thao] Page 1: Found 39 candidate articles\n",
      "[the-thao] Page 1: Added 21 articles (total: 21)\n",
      "[the-thao] Fetching page 2: https://laodong.vn/the-thao?page=2\n",
      "[the-thao] Page 2: Found 42 candidate articles\n",
      "[the-thao] Page 2: Added 7 articles (total: 28)\n",
      "[the-thao] Fetching page 3: https://laodong.vn/the-thao?page=3\n",
      "[the-thao] Page 3: Found 42 candidate articles\n",
      "[the-thao] Page 3: Added 0 articles (total: 28)\n",
      "[the-thao] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/the-thao/] Added 28 articles\n",
      "\n",
      "\n",
      "[suc-khoe] Starting crawl...\n",
      "[suc-khoe] Fetching page 1: https://laodong.vn/suc-khoe/\n",
      "[suc-khoe] Page 1: Found 38 candidate articles\n",
      "[suc-khoe] Page 1: Added 22 articles (total: 22)\n",
      "[suc-khoe] Fetching page 2: https://laodong.vn/suc-khoe?page=2\n",
      "[suc-khoe] Page 2: Found 38 candidate articles\n",
      "[suc-khoe] Page 2: Added 10 articles (total: 32)\n",
      "[suc-khoe] Fetching page 3: https://laodong.vn/suc-khoe?page=3\n",
      "[suc-khoe] Page 3: Found 39 candidate articles\n",
      "[suc-khoe] Page 3: Added 0 articles (total: 32)\n",
      "[suc-khoe] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/suc-khoe/] Added 32 articles\n",
      "\n",
      "\n",
      "[cong-nghe] Starting crawl...\n",
      "[cong-nghe] Fetching page 1: https://laodong.vn/cong-nghe/\n",
      "[cong-nghe] Page 1: Found 31 candidate articles\n",
      "[cong-nghe] Page 1: Added 7 articles (total: 7)\n",
      "[cong-nghe] Fetching page 2: https://laodong.vn/cong-nghe?page=2\n",
      "[cong-nghe] Page 2: Found 33 candidate articles\n",
      "[cong-nghe] Page 2: Added 1 articles (total: 8)\n",
      "[cong-nghe] Fetching page 3: https://laodong.vn/cong-nghe?page=3\n",
      "[cong-nghe] Page 3: Found 32 candidate articles\n",
      "[cong-nghe] Page 3: Added 5 articles (total: 13)\n",
      "[cong-nghe] Fetching page 4: https://laodong.vn/cong-nghe?page=4\n",
      "[cong-nghe] Page 4: Found 33 candidate articles\n",
      "[cong-nghe] Page 4: Added 1 articles (total: 14)\n",
      "[cong-nghe] Fetching page 5: https://laodong.vn/cong-nghe?page=5\n",
      "[cong-nghe] Page 5: Found 33 candidate articles\n",
      "[cong-nghe] Page 5: Added 0 articles (total: 14)\n",
      "[cong-nghe] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/cong-nghe/] Added 14 articles\n",
      "\n",
      "\n",
      "[xe] Starting crawl...\n",
      "[xe] Fetching page 1: https://laodong.vn/xe/\n",
      "[xe] Page 1: Found 34 candidate articles\n",
      "[xe] Page 1: Added 5 articles (total: 5)\n",
      "[xe] Fetching page 2: https://laodong.vn/xe?page=2\n",
      "[xe] Page 2: Found 31 candidate articles\n",
      "[xe] Page 2: Added 2 articles (total: 7)\n",
      "[xe] Fetching page 3: https://laodong.vn/xe?page=3\n",
      "[xe] Page 3: Found 34 candidate articles\n",
      "[xe] Page 3: Added 0 articles (total: 7)\n",
      "[xe] All articles older than 2026-01-20, stopping\n",
      "‚úì [https://laodong.vn/xe/] Added 7 articles\n",
      "\n",
      "\n",
      "[du-lich] Starting crawl...\n",
      "[du-lich] Fetching page 1: https://laodong.vn/du-lich/\n",
      "[du-lich] Page 1: No articles found, stopping\n",
      "‚úì [https://laodong.vn/du-lich/] Added 0 articles\n",
      "\n",
      "\n",
      "============================================================\n",
      "Done! Total appended 331 rows to laodong_html_articles_vi.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# LaoDong HTML Crawler - Crawl theo category pages v·ªõi pagination\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from urllib.parse import urlparse, urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "VN_TZ = timezone(timedelta(hours=7))\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATEGORY_URLS = [\n",
    "    \"https://laodong.vn/thoi-su/\",\n",
    "    \"https://laodong.vn/the-gioi/\",\n",
    "    \"https://laodong.vn/xa-hoi/\",\n",
    "    \"https://laodong.vn/phap-luat/\",\n",
    "    \"https://laodong.vn/kinh-doanh/\",\n",
    "    \"https://laodong.vn/bat-dong-san/\",\n",
    "    \"https://laodong.vn/van-hoa/\",\n",
    "    \"https://laodong.vn/giao-duc/\",\n",
    "    \"https://laodong.vn/the-thao/\",\n",
    "    \"https://laodong.vn/suc-khoe/\",\n",
    "    \"https://laodong.vn/cong-nghe/\",\n",
    "    \"https://laodong.vn/xe/\",\n",
    "    \"https://laodong.vn/du-lich/\",\n",
    "]\n",
    "\n",
    "# Crawl t·ª´ m·ªõi -> c≈© cho t·ªõi khi b√†i c√≥ ng√†y < END_DATE\n",
    "END_DATE = \"2026-01-20\"  # YYYY-MM-DD - L·∫•y t·ª´ th√°ng 12/2025 ƒë·ªÉ c√≥ nhi·ªÅu d·ªØ li·ªáu h∆°n\n",
    "MAX_PAGES_PER_CATEGORY = 100  # M·ªói category t·ªëi ƒëa 100 trang\n",
    "\n",
    "CSV_PATH = \"laodong_html_articles_vi.csv\"\n",
    "\n",
    "TIMEOUT = 25\n",
    "REQUEST_DELAY_BASE = 0.3\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "# ===========================================\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    \"source.name\",\n",
    "    \"url\",\n",
    "    \"language\",\n",
    "    \"category.primary\",\n",
    "    \"keywords\",\n",
    "    \"entities\",\n",
    "    \"content.text\",\n",
    "]\n",
    "\n",
    "SOURCE_NAME = \"LaoDong\"\n",
    "DEFAULT_LANGUAGE = \"vi\"\n",
    "DEBUG = False\n",
    "\n",
    "# ----- HTTP session with retry -----\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "retry = Retry(\n",
    "    total=6,\n",
    "    connect=6,\n",
    "    read=6,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\", \"HEAD\"],\n",
    "    respect_retry_after_header=True,\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def log(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(REQUEST_DELAY_BASE + random.uniform(0, 0.4))\n",
    "\n",
    "\n",
    "def md5_id(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def fetch_with_cookie_handling(url: str) -> requests.Response:\n",
    "    \"\"\"Fetch URL with laodong.vn cookie protection handling\"\"\"\n",
    "    r = session.get(url, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    # Check if response is cookie-setting JavaScript\n",
    "    if \"document.cookie\" in r.text and len(r.content) < 500:\n",
    "        match = re.search(r'document\\.cookie=\"([^\"]+)\"', r.text)\n",
    "        if match:\n",
    "            cookie_str = match.group(1)\n",
    "            cookie_parts = cookie_str.split(\"=\", 1)\n",
    "            if len(cookie_parts) == 2:\n",
    "                cookie_name, cookie_value = cookie_parts\n",
    "                session.cookies.set(cookie_name, cookie_value)\n",
    "                log(f\"[DEBUG] Set cookie: {cookie_name}={cookie_value[:20]}...\")\n",
    "        \n",
    "        polite_sleep()\n",
    "        r = session.get(url, timeout=TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "    \n",
    "    return r\n",
    "\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    return fetch_with_cookie_handling(url).text\n",
    "\n",
    "\n",
    "def to_iso_utc(s: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Convert datetime string to ISO UTC format\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(s)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=VN_TZ)\n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_local_date(iso_utc: str) -> Optional[str]:\n",
    "    \"\"\"Convert ISO UTC to local date YYYY-MM-DD\"\"\"\n",
    "    if not iso_utc:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(iso_utc)\n",
    "        if not dt:\n",
    "            return None\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        dt_local = dt.astimezone(VN_TZ)\n",
    "        return dt_local.date().isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_csv_header(csv_path: str):\n",
    "    if not os.path.exists(csv_path) or os.path.getsize(csv_path) == 0:\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(CSV_HEADER)\n",
    "\n",
    "\n",
    "def load_seen_from_csv(csv_path: str) -> Tuple[Set[str], Set[str]]:\n",
    "    seen_urls, seen_ids = set(), set()\n",
    "    if not os.path.exists(csv_path):\n",
    "        return seen_urls, seen_ids\n",
    "    try:\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r, None)\n",
    "            if not header:\n",
    "                return seen_urls, seen_ids\n",
    "            id_idx = header.index(\"id\") if \"id\" in header else 0\n",
    "            url_idx = header.index(\"url\") if \"url\" in header else 4\n",
    "            for row in r:\n",
    "                if len(row) > url_idx:\n",
    "                    u = row[url_idx].strip()\n",
    "                    if u:\n",
    "                        seen_urls.add(u)\n",
    "                if len(row) > id_idx:\n",
    "                    i = row[id_idx].strip()\n",
    "                    if i:\n",
    "                        seen_ids.add(i)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return seen_urls, seen_ids\n",
    "\n",
    "\n",
    "def append_row(csv_path: str, row: Dict[str, Any]):\n",
    "    with open(csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([row.get(k, \"\") for k in CSV_HEADER])\n",
    "        f.flush()\n",
    "\n",
    "\n",
    "def extract_article_meta(article_html: str) -> Dict[str, Any]:\n",
    "    soup = BeautifulSoup(article_html, \"lxml\")\n",
    "\n",
    "    # title\n",
    "    title = \"\"\n",
    "    og = soup.select_one('meta[property=\"og:title\"]')\n",
    "    if og and og.get(\"content\"):\n",
    "        title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        h1 = soup.select_one(\"h1\")\n",
    "        if h1:\n",
    "            title = h1.get_text(strip=True)\n",
    "\n",
    "    # published_at\n",
    "    pub = \"\"\n",
    "    m_pub = soup.select_one('meta[property=\"article:published_time\"]')\n",
    "    if m_pub and m_pub.get(\"content\"):\n",
    "        pub = to_iso_utc(m_pub[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        m2 = soup.select_one('meta[itemprop=\"datePublished\"]')\n",
    "        if m2 and m2.get(\"content\"):\n",
    "            pub = to_iso_utc(m2[\"content\"].strip()) or \"\"\n",
    "    if not pub:\n",
    "        ttag = soup.select_one(\"time\")\n",
    "        if ttag:\n",
    "            pub = to_iso_utc(ttag.get(\"datetime\") or ttag.get_text(strip=True)) or \"\"\n",
    "\n",
    "    # category\n",
    "    category_primary = \"\"\n",
    "    sec = soup.select_one('meta[property=\"article:section\"]')\n",
    "    if sec and sec.get(\"content\"):\n",
    "        category_primary = sec[\"content\"].strip()\n",
    "\n",
    "    # language\n",
    "    language = DEFAULT_LANGUAGE\n",
    "    html_tag = soup.find(\"html\")\n",
    "    if html_tag:\n",
    "        lang = html_tag.get(\"lang\")\n",
    "        if lang:\n",
    "            language = lang.lower().strip()\n",
    "\n",
    "    # keywords\n",
    "    keywords = []\n",
    "    kw = soup.select_one('meta[name=\"keywords\"]')\n",
    "    if kw and kw.get(\"content\"):\n",
    "        keywords = [x.strip() for x in kw[\"content\"].split(\",\") if x.strip()]\n",
    "\n",
    "    # content.text\n",
    "    content_text = \"\"\n",
    "    article_body = soup.select_one(\"article.detail-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\".detail-content\")\n",
    "    if not article_body:\n",
    "        article_body = soup.select_one(\"article\")\n",
    "    \n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all(\"p\")\n",
    "        text_parts = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                text_parts.append(text)\n",
    "        content_text = \" \".join(text_parts)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"published_at\": pub,\n",
    "        \"language\": language,\n",
    "        \"keywords\": keywords,\n",
    "        \"category_from_article\": category_primary,\n",
    "        \"entities\": [],\n",
    "        \"content_text\": content_text,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_article_urls_from_page(html: str, category_url: str) -> List[str]:\n",
    "    \"\"\"Extract article URLs from category page\"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    urls = []\n",
    "    \n",
    "    # LaoDong uses various link patterns\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        \n",
    "        # Make absolute URL\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://laodong.vn\" + href\n",
    "        \n",
    "        # Only laodong.vn articles\n",
    "        if not href.startswith(\"https://laodong.vn/\"):\n",
    "            continue\n",
    "        \n",
    "        # Article URLs end with .ldo\n",
    "        if not href.endswith(\".ldo\"):\n",
    "            continue\n",
    "        \n",
    "        # Remove query params\n",
    "        href = href.split(\"?\")[0]\n",
    "        urls.append(href)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            result.append(u)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_next_page_url(category_url: str, page: int) -> str:\n",
    "    \"\"\"\n",
    "    LaoDong pagination: ?page=2, ?page=3, etc.\n",
    "    \"\"\"\n",
    "    base_url = category_url.rstrip(\"/\")\n",
    "    return f\"{base_url}?page={page}\"\n",
    "\n",
    "\n",
    "def make_row(url: str, meta: Dict[str, Any], category_fallback: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": md5_id(url),\n",
    "        \"title\": meta.get(\"title\") or \"\",\n",
    "        \"published_at\": meta.get(\"published_at\") or \"\",\n",
    "        \"source.name\": SOURCE_NAME,\n",
    "        \"url\": url,\n",
    "        \"language\": meta.get(\"language\") or DEFAULT_LANGUAGE,\n",
    "        \"category.primary\": (meta.get(\"category_from_article\") or category_fallback) or \"\",\n",
    "        \"keywords\": \"|\".join(meta.get(\"keywords\") or []),\n",
    "        \"entities\": \"|\".join(meta.get(\"entities\") or []),\n",
    "        \"content.text\": meta.get(\"content_text\") or \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def category_slug_from_url(url: str) -> str:\n",
    "    \"\"\"Extract category slug from URL\"\"\"\n",
    "    path = urlparse(url).path.strip(\"/\")\n",
    "    return path.split(\"/\")[0] if \"/\" in path else path\n",
    "\n",
    "\n",
    "def crawl_category(category_url: str, end_date: str, seen_urls: Set[str], seen_ids: Set[str]) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Crawl one category from new to old until < end_date\n",
    "    Returns: (added, skipped_duplicate, skipped_old)\n",
    "    \"\"\"\n",
    "    added = 0\n",
    "    skipped_duplicate = 0\n",
    "    skipped_old = 0\n",
    "    page = 1\n",
    "    category_slug = category_slug_from_url(category_url)\n",
    "    \n",
    "    print(f\"\\n[{category_slug}] Starting crawl...\")\n",
    "    \n",
    "    while page <= MAX_PAGES_PER_CATEGORY:\n",
    "        # Page 1 is the category URL, page 2+ use ?page=N\n",
    "        if page == 1:\n",
    "            url_page = category_url\n",
    "        else:\n",
    "            url_page = get_next_page_url(category_url, page)\n",
    "        \n",
    "        print(f\"[{category_slug}] Fetching page {page}: {url_page}\")\n",
    "        \n",
    "        try:\n",
    "            html = fetch_text(url_page)\n",
    "        except Exception as e:\n",
    "            print(f\"[{category_slug}] Page {page} fetch failed: {e}\")\n",
    "            break\n",
    "        \n",
    "        article_urls = extract_article_urls_from_page(html, category_url)\n",
    "        \n",
    "        if not article_urls:\n",
    "            print(f\"[{category_slug}] Page {page}: No articles found, stopping\")\n",
    "            break\n",
    "        \n",
    "        print(f\"[{category_slug}] Page {page}: Found {len(article_urls)} candidate articles\")\n",
    "        \n",
    "        page_all_older_than_end = True\n",
    "        page_added = 0\n",
    "        \n",
    "        for aurl in article_urls:\n",
    "            if aurl in seen_urls:\n",
    "                continue\n",
    "            \n",
    "            aid = md5_id(aurl)\n",
    "            if aid in seen_ids:\n",
    "                continue\n",
    "            \n",
    "            # Fetch article\n",
    "            try:\n",
    "                ah = fetch_text(aurl)\n",
    "                meta = extract_article_meta(ah)\n",
    "            except Exception as e:\n",
    "                log(f\"[WARN] Article fetch failed {aurl}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                polite_sleep()\n",
    "            \n",
    "            pub_iso = meta.get(\"published_at\") or \"\"\n",
    "            pub_local_date = iso_to_local_date(pub_iso) or \"\"\n",
    "            \n",
    "            # Check if article is old enough to stop\n",
    "            if pub_local_date and pub_local_date < end_date:\n",
    "                pass  # B√†i c≈©, kh√¥ng ghi\n",
    "            else:\n",
    "                page_all_older_than_end = False\n",
    "            \n",
    "            # Only save articles >= end_date\n",
    "            if (not pub_local_date) or (pub_local_date >= end_date):\n",
    "                row = make_row(aurl, meta, category_fallback=category_slug)\n",
    "                append_row(CSV_PATH, row)\n",
    "                seen_urls.add(aurl)\n",
    "                seen_ids.add(aid)\n",
    "                added += 1\n",
    "                page_added += 1\n",
    "        \n",
    "        print(f\"[{category_slug}] Page {page}: Added {page_added} articles (total: {added})\")\n",
    "        \n",
    "        # Stop if all articles on this page are older than end_date\n",
    "        if page_all_older_than_end:\n",
    "            print(f\"[{category_slug}] All articles older than {end_date}, stopping\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        polite_sleep()\n",
    "    \n",
    "    return added\n",
    "\n",
    "\n",
    "def main():\n",
    "    ensure_csv_header(CSV_PATH)\n",
    "    seen_urls, seen_ids = load_seen_from_csv(CSV_PATH)\n",
    "    \n",
    "    print(f\"Starting LaoDong HTML crawler...\")\n",
    "    print(f\"END_DATE: {END_DATE}\")\n",
    "    print(f\"CSV: {CSV_PATH}\")\n",
    "    print(f\"Categories: {len(CATEGORY_URLS)}\")\n",
    "    \n",
    "    total = 0\n",
    "    for cat in CATEGORY_URLS:\n",
    "        try:\n",
    "            added = crawl_category(cat, END_DATE, seen_urls, seen_ids)\n",
    "            print(f\"‚úì [{cat}] Added {added} articles\\n\")\n",
    "            total += added\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó [{cat}] ERROR: {e}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Done! Total appended {total} rows to {CSV_PATH}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
