# ğŸ“° Vietnamese News Data

Dá»¯ liá»‡u tin tá»©c tá»« cÃ¡c bÃ¡o lá»›n Viá»‡t Nam, tá»± Ä‘á»™ng cáº­p nháº­t má»—i 6 giá».

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Äá»c trá»±c tiáº¿p tá»« URL vá»›i Python

```python
import pandas as pd

# Base URL
BASE_URL = "https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/"

# Äá»c tá»«ng file
df_tonghop = pd.read_csv(BASE_URL + "rss_feed_articles_v2.csv")
df_laodong = pd.read_csv(BASE_URL + "laodong_html_articles_vi.csv")
df_znews = pd.read_csv(BASE_URL + "znews_html_categories_vi.csv")
df_24h = pd.read_csv(BASE_URL + "24h_html_categories_vi.csv")

print(f"Tá»•ng há»£p: {len(df_tonghop):,} bÃ i")
print(f"Lao Äá»™ng: {len(df_laodong):,} bÃ i")
print(f"ZNews: {len(df_znews):,} bÃ i")
print(f"24h: {len(df_24h):,} bÃ i")
```

## ğŸ“Š Direct Download Links

| Nguá»“n | Link |
|-------|------|
| **Tá»•ng há»£p RSS** | [Download CSV](https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv) |
| **Lao Äá»™ng** | [Download CSV](https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/laodong_html_articles_vi.csv) |
| **ZNews** | [Download CSV](https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/znews_html_categories_vi.csv) |
| **24h** | [Download CSV](https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/24h_html_categories_vi.csv) |

## ğŸ”„ Cáº­p nháº­t

Dá»¯ liá»‡u tá»± Ä‘á»™ng cáº­p nháº­t **má»—i 6 giá»** qua GitHub Actions.

## ğŸ“ Cáº¥u trÃºc dá»¯ liá»‡u

``` Columns:
- id              : Unique identifier (MD5 hash)
- title           : TiÃªu Ä‘á» bÃ i viáº¿t
- published_at    : Thá»i gian xuáº¥t báº£n (ISO 8601 UTC)
- source.name     : Nguá»“n tin (VNExpress, Thanh NiÃªn, Tuá»•i Tráº», etc.)
- url             : Link bÃ i viáº¿t gá»‘c
- language        : NgÃ´n ngá»¯ (vi)
- category.primary: ChuyÃªn má»¥c chÃ­nh
- keywords        : Tá»« khÃ³a (phÃ¢n cÃ¡ch bá»Ÿi |)
- entities        : Thá»±c thá»ƒ Ä‘Æ°á»£c trÃ­ch xuáº¥t (phÃ¢n cÃ¡ch bá»Ÿi |)
- content.text    : Ná»™i dung Ä‘áº§y Ä‘á»§ cá»§a bÃ i viáº¿t
```

## ğŸ’¡ VÃ­ dá»¥ nÃ¢ng cao

### Merge táº¥t cáº£ nguá»“n

```python
import pandas as pd

BASE_URL = "https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/"

files = {
    'tonghop': 'rss_feed_articles_v2.csv',
    'laodong': 'laodong_html_articles_vi.csv',
    'znews': 'znews_html_categories_vi.csv',
    '24h': '24h_html_categories_vi.csv'
}

dfs = []
for source, filename in files.items():
    df = pd.read_csv(BASE_URL + filename)
    df['data_source'] = source
    dfs.append(df)
    print(f"âœ“ Loaded {len(df):,} articles from {source}")

# Merge táº¥t cáº£
df_all = pd.concat(dfs, ignore_index=True)
df_all['published_at'] = pd.to_datetime(df_all['published_at'])

print(f"\nğŸ“Š Total: {len(df_all):,} articles")
```

### PhÃ¢n tÃ­ch tá»« khÃ³a phá»• biáº¿n

```python
import pandas as pd
from collections import Counter

df = pd.read_csv("https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv")

# Extract all keywords
all_keywords = []
for kw_str in df['keywords'].dropna():
    keywords = kw_str.split('|')
    all_keywords.extend(keywords)

# Count top keywords
keyword_counts = Counter(all_keywords)
print("Top 20 keywords:")
for keyword, count in keyword_counts.most_common(20):
    print(f"{keyword:<30} {count:>5,}")
```

### PhÃ¢n tÃ­ch xu hÆ°á»›ng theo thá»i gian

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv")

# Convert to datetime
df['published_at'] = pd.to_datetime(df['published_at'])
df['date'] = df['published_at'].dt.date

# Count articles per day
daily_counts = df['date'].value_counts().sort_index()

# Plot
plt.figure(figsize=(12, 6))
daily_counts.plot(kind='line', marker='o')
plt.title('Sá»‘ lÆ°á»£ng bÃ i viáº¿t theo ngÃ y', fontsize=16)
plt.xlabel('NgÃ y')
plt.ylabel('Sá»‘ bÃ i viáº¿t')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### PhÃ¢n tÃ­ch theo nguá»“n tin

```python
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv")

# Count by source
source_counts = df['source.name'].value_counts()

print("PhÃ¢n bá»‘ theo nguá»“n:")
for source, count in source_counts.items():
    percentage = count/len(df)*100
    print(f"{source:<30} {count:>6,} bÃ i ({percentage:.1f}%)")
```

### Lá»c bÃ i viáº¿t theo tá»« khÃ³a

```python
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv")

# TÃ¬m bÃ i viáº¿t cÃ³ chá»©a tá»« khÃ³a "kinh táº¿"
keyword = "kinh táº¿"
filtered = df[df['keywords'].str.contains(keyword, case=False, na=False)]

print(f"TÃ¬m tháº¥y {len(filtered)} bÃ i vá» '{keyword}':")
for idx, row in filtered.head(10).iterrows():
    print(f"\n- {row['title']}")
    print(f"  Nguá»“n: {row['source.name']} | {row['published_at']}")
    print(f"  Link: {row['url']}")
```

## ğŸ” Google Colab Example

```python
# Cháº¡y trong Google Colab
!pip install pandas matplotlib -q

import pandas as pd

# Load data
url = "https://raw.githubusercontent.com/leduchuy20/crawl_news/main/crawl_news/crawl/rss_feed_articles_v2.csv"
df = pd.read_csv(url)

# Quick stats
print(f"Total articles: {len(df):,}")
print(f"\nTop 10 sources:")
print(df['source.name'].value_counts().head(10))

print(f"\nTop 10 categories:")
print(df['category.primary'].value_counts().head(10))
```

## ğŸ› ï¸ Technologies

- **Python**: Crawling & data processing
- **Pandas**: Data manipulation
- **GitHub Actions**: Automated scheduling
- **Docker**: Containerization (optional)

## ğŸ“‚ Project Structure

```
crawl_news/
â”œâ”€â”€ crawl/
â”‚   â”œâ”€â”€ all_crawl.ipynb          # Main crawler notebook
â”‚   â”œâ”€â”€ rss_feed_articles_v2.csv # Aggregated RSS data
â”‚   â”œâ”€â”€ laodong_html_articles_vi.csv
â”‚   â”œâ”€â”€ znews_html_categories_vi.csv
â”‚   â””â”€â”€ 24h_html_categories_vi.csv
â”œâ”€â”€ crawl_news/                   # Individual crawler notebooks
â”œâ”€â”€ dags/                         # Airflow DAGs (if using)
â”œâ”€â”€ jobs/                         # Spark jobs (if using)
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ docker-compose.yml            # Docker setup
```

## ğŸ“‹ Requirements

```
pandas
feedparser
beautifulsoup4
requests
```

## ğŸ” Privacy & Legal

- Dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p tá»« cÃ¡c nguá»“n cÃ´ng khai (RSS feeds)
- Chá»‰ sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch nghiÃªn cá»©u vÃ  giÃ¡o dá»¥c
- KhÃ´ng sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch thÆ°Æ¡ng máº¡i
- TÃ´n trá»ng báº£n quyá»n cá»§a cÃ¡c trang tin gá»‘c

## ğŸ“œ License

MIT License - Free to use for research and education.

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“§ Contact

- GitHub: [@leduchuy20](https://github.com/leduchuy20)
- Repository: [crawl_news](https://github.com/leduchuy20/crawl_news)

## ğŸ“ˆ Stats

![GitHub stars](https://img.shields.io/github/stars/leduchuy20/crawl_news?style=social)
![GitHub forks](https://img.shields.io/github/forks/leduchuy20/crawl_news?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/leduchuy20/crawl_news?style=social)

---

â­ **Star this repo** if you find it useful!

**Last updated:** Auto-updated every 6 hours via GitHub Actions
